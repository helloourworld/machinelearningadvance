---
layout: post
title: Spark Streaming Lesson 1
category: Hadoop
catalog: yes
description: Spark Streaming Programming Guide.
catalog: yes
tags:
    - Spark
---

### 1 Overview

Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput(高吞吐), fault-tolerant(容错机制) stream processing of live data streams(实时流处理). Data can be ingested from many sources like Kafka, Flume, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like **map, reduce, join and window**. Finally, processed data can be pushed out to filesystems, databases, and live dashboards(现场仪表盘). In fact, you can apply Spark’s [machine learning](http://spark.apache.org/docs/latest/ml-guide.html) and [graph processing algorithms](http://spark.apache.org/docs/latest/graphx-programming-guide.html) on data streams.
![](/images/hadoop/streaming-arch.png)

Internally, it works as follows. Spark Streaming receives live input data streams and divides the data into batches, which are then processed by the Spark engine to generate the final stream of results in batches.(Spark Streaming在内部的处理机制是，接收实时流的数据，并根据一定的时间间隔拆分成一批批的数据，然后通过Spark Engine处理这些批数据，最终得到处理后的一批批结果数据。)
![](/images/hadoop/streaming-flow.png)

Spark Streaming provides a high-level abstraction called **discretized stream or DStream**, which represents a continuous stream of data. DStreams can be created either from input data streams from sources such as Kafka, Flume, and Kinesis, or by applying high-level operations on other DStreams. Internally, a DStream is represented as a **sequence of [RDDs](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD)**.

### 2 A Quick Example

~~~scala
18 // scalastyle:off println
19 package org.apache.spark.examples.streaming
20
21 import org.apache.spark.SparkConf
22 import org.apache.spark.storage.StorageLevel
23 import org.apache.spark.streaming.{Seconds, StreamingContext}
24
25 /**
26  * Counts words in UTF8 encoded, '\n' delimited text received from the network every second.
27  *
28  * Usage: NetworkWordCount <hostname> <port>
29  * <hostname> and <port> describe the TCP server that Spark Streaming would connect to receive data.
30  *
31  * To run this on your local machine, you need to first run a Netcat server
32  *    `$ nc -lk 9999`
33  * and then run the example
34  *    `$ bin/run-example org.apache.spark.examples.streaming.NetworkWordCount localhost 9999`
35  */
36 object NetworkWordCount {
37   def main(args: Array[String]) {
38     if (args.length < 2) {
39       System.err.println("Usage: NetworkWordCount <hostname> <port>")
40       System.exit(1)
41     }
42
43     StreamingExamples.setStreamingLogLevels()
44
45     // Create the context with a 1 second batch size
46     val sparkConf = new SparkConf().setAppName("NetworkWordCount")
47     val ssc = new StreamingContext(sparkConf, Seconds(1))
48
49     // Create a socket stream on target ip:port and count the
50     // words in input stream of \n delimited text (eg. generated by 'nc')
51     // Note that no duplication in storage level only for running locally.
52     // Replication necessary in distributed scenario for fault tolerance.
53     val lines = ssc.socketTextStream(args(0), args(1).toInt, StorageLevel.MEMORY_AND_DISK_SER)
54     val words = lines.flatMap(_.split(" "))
55     val wordCounts = words.map(x => (x, 1)).reduceByKey(_ + _)
56     wordCounts.print()
57     ssc.start()
58     ssc.awaitTermination()
59   }
60 }
61 // scalastyle:on println
~~~

~~~scala
[hadoop@NN01 ~]$ nc -lp 9999
Hello world
This is david
stop now
~~~

~~~scala
[hadoop@NN01 spark]$ ./bin/run-example streaming.NetworkWordCount NN01.HadoopVM 9999
-------------------------------------------
Time: 1473325234000 ms
-------------------------------------------
(is,1)
(Hello,1)
(This,1)
(david,1)
(world,1)
...
Time: 1473325243000 ms
-------------------------------------------
(stop,1)
(now,1)
~~~

### 3 Basic Concepts

#### 3.1 Linking

Similar to Spark, Spark Streaming is available through Maven Central. To write your own Spark Streaming program, you will have to add the following dependency to your SBT or Maven project.

<div class="codetabs">

<div data-lang="Maven">

    <pre><code>&lt;dependency&gt;
    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
    &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt;
    &lt;version&gt;2.0.0&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
  </div>

<div data-lang="SBT">

    <pre><code>libraryDependencies += "org.apache.spark" % "spark-streaming_2.11" % "2.0.0"
</code></pre>
  </div>
</div>


#### 3.2 Initializing StreamingContext

#### 3.3 Discretized Streams(DStreams)

#### 3.4 Input Dstreams and Receivers

#### 3.5 Transformations on DStreams

#### 3.6 Accumulators and Broadcast Variables

#### 3.7 DataFrame and SQL Operations

#### 3.8 MLlib Operations

#### 3.9 Caching / Persistence

#### 3.10 Checkpointing


































































### Ref: nc input & output

~~~shell
[hadoop@NN01 ~]$ nc -lp 9999
hhh # input
this is a test # input
hello #output
~~~

~~~shell
[hadoop@NN01 ~]$ nc 127.0.0.1 9999
hhh
this is a test
hello
~~~
