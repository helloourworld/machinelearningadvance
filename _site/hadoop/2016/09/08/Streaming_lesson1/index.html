<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="baidu-site-verification" content="g89MuzujW3" />
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="关于机器学习，关于大数据，关于你身边的金融，关于我们的生活... | LijunYu">
    <meta name="keywords"  content="LijunYu, 包包的老公, 大数据, 机器学习, CFA, Machine Learning(Deep Learning), Big Data, Spark">
    <link rel="stylesheet" href="/css/default.css" type="text/css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
    <link rel="alternate" type="application/atom+xml" title="Recent Entries" href="/atom.xml" />
    <link rel="alternatecss" type="application/rss+xml" title="“Lijun Yu's Blog”" href="http://helloourworld.github.io/feed.xml">
    <script src="/js/jquery-1.7.1.min.js" type="text/javascript"></script>

    <title>Spark Streaming Lesson 1 - Maching Learning | 机器学习笔记</title>

    <link rel="canonical" href="http://localhost:4000/hadoop/2016/09/08/Streaming_lesson1/">

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/hux-blog.min.css">

    <!-- Pygments Github CSS -->
    <link rel="stylesheet" href="/css/syntax.css">

    <!-- Custom Fonts -->
    <link href="http://cdn.staticfile.org/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">

    <!-- ga & ba script hoook -->
    <script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>

</head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">

    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Big Data Memo</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>
                    
                    <li>
                        <a href="/2018-7-17-gongzonghao_yu/">数据挖掘，你我常常忽略的小问题</a>
                    </li>
                    
                    <li>
                        <a href="/about/">About</a>
                    </li>
                    
                    <li>
                        <a href="/category/">Category</a>
                    </li>
                    
                    <li>
                        <a href="/tags/">Tags</a>
                    </li>
                    
                    <li>
                        <a href="/talks/">Talks</a>
                    </li>
                    
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    var __HuxNav__ = {
        close: function(){
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        },
        open: function(){
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }

    // Bind Event
    $toggle.addEventListener('click', function(e){
        if ($navbar.className.indexOf('in') > 0) {
            __HuxNav__.close()
        }else{
            __HuxNav__.open()
        }
    })

    /**
     * Since Fastclick is used to delegate 'touchstart' globally
     * to hack 300ms delay in iOS by performing a fake 'click',
     * Using 'e.stopPropagation' to stop 'touchstart' event from 
     * $toggle/$collapse will break global delegation.
     * 
     * Instead, we use a 'e.target' filter to prevent handler
     * added to document close HuxNav.  
     *
     * Also, we use 'click' instead of 'touchstart' as compromise
     */
    document.addEventListener('click', function(e){
        if(e.target == $toggle) return;
        if(e.target.className == 'icon-bar') return;
        __HuxNav__.close();
    })
</script>


    <!-- Image to hack wechat -->
<!-- <img src="/images/background.jpg" width="0" height="0"> -->

<!-- Post Header -->
<style type="text/css">
    header.intro-header{
        position: relative;
        background-image: url('/images/background.jpg')
    }

    
</style>
<header class="intro-header" >
    <div class="header-mask"></div>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <div class="tags">
                        
                        <a class="tag" href="/tags/#Spark" title="Spark">Spark</a>
                        
                    </div>
                    <h1>Spark Streaming Lesson 1</h1>
                    
                    
                    <h2 class="subheading"></h2>
                    
                    <span class="meta">Posted by Big Data Memo on September 8, 2016</span>
                </div>
            </div>
        </div>
    </div>
</header>

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

    <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

				<h3 id="1-overview">1 Overview</h3>

<p>Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput(高吞吐), fault-tolerant(容错机制) stream processing of live data streams(实时流处理). Data can be ingested from many sources like Kafka, Flume, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like <strong>map, reduce, join and window</strong>. Finally, processed data can be pushed out to filesystems, databases, and live dashboards(现场仪表盘). In fact, you can apply Spark’s <a href="http://spark.apache.org/docs/latest/ml-guide.html">machine learning</a> and <a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">graph processing algorithms</a> on data streams.
<img src="/images/hadoop/streaming-arch.png" alt="" /></p>

<p>Internally, it works as follows. Spark Streaming receives live input data streams and divides the data into batches, which are then processed by the Spark engine to generate the final stream of results in batches.(Spark Streaming在内部的处理机制是，接收实时流的数据，并根据一定的时间间隔拆分成一批批的数据，然后通过Spark Engine处理这些批数据，最终得到处理后的一批批结果数据。)
<img src="/images/hadoop/streaming-flow.png" alt="" /></p>

<p>Spark Streaming provides a high-level abstraction called <strong>discretized stream or DStream</strong>, which represents a continuous stream of data. DStreams can be created either from input data streams from sources such as Kafka, Flume, and Kinesis, or by applying high-level operations on other DStreams. Internally, a DStream is represented as a <strong>sequence of <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD">RDDs</a></strong>.see <a href="/hadoop/2016/09/08/Streaming_lesson1/#discretized-streamsdstreams">Dstream</a>.</p>

<h3 id="2-a-quick-example">2 A Quick Example</h3>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="mi">18</span> <span class="c1">// scalastyle:off println
</span><span class="mi">19</span> <span class="k">package</span> <span class="nn">org.apache.spark.examples.streaming</span>
<span class="mi">20</span>
<span class="mi">21</span> <span class="k">import</span> <span class="nn">org.apache.spark.SparkConf</span>
<span class="mi">22</span> <span class="k">import</span> <span class="nn">org.apache.spark.storage.StorageLevel</span>
<span class="mi">23</span> <span class="k">import</span> <span class="nn">org.apache.spark.streaming.</span><span class="o">{</span><span class="nc">Seconds</span><span class="o">,</span> <span class="nc">StreamingContext</span><span class="o">}</span>
<span class="mi">24</span>
<span class="mi">25</span> <span class="cm">/**
26  * Counts words in UTF8 encoded, '\n' delimited text received from the network every second.
27  *
28  * Usage: NetworkWordCount &lt;hostname&gt; &lt;port&gt;
29  * &lt;hostname&gt; and &lt;port&gt; describe the TCP server that Spark Streaming would connect to receive data.
30  *
31  * To run this on your local machine, you need to first run a Netcat server
32  *    `$ nc -lk 9999`
33  * and then run the example
34  *    `$ bin/run-example org.apache.spark.examples.streaming.NetworkWordCount localhost 9999`
35  */</span>
<span class="mi">36</span> <span class="k">object</span> <span class="nc">NetworkWordCount</span> <span class="o">{</span>
<span class="mi">37</span>   <span class="k">def</span> <span class="nf">main</span><span class="o">(</span><span class="n">args</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span> <span class="o">{</span>
<span class="mi">38</span>     <span class="nf">if</span> <span class="o">(</span><span class="nv">args</span><span class="o">.</span><span class="py">length</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="o">)</span> <span class="o">{</span>
<span class="mi">39</span>       <span class="nv">System</span><span class="o">.</span><span class="py">err</span><span class="o">.</span><span class="py">println</span><span class="o">(</span><span class="s">"Usage: NetworkWordCount &lt;hostname&gt; &lt;port&gt;"</span><span class="o">)</span>
<span class="mi">40</span>       <span class="nv">System</span><span class="o">.</span><span class="py">exit</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
<span class="mi">41</span>     <span class="o">}</span>
<span class="mi">42</span>
<span class="mi">43</span>     <span class="nv">StreamingExamples</span><span class="o">.</span><span class="py">setStreamingLogLevels</span><span class="o">()</span>
<span class="mi">44</span>
<span class="mi">45</span>     <span class="c1">// Create the context with a 1 second batch size
</span><span class="mi">46</span>     <span class="k">val</span> <span class="nv">sparkConf</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SparkConf</span><span class="o">().</span><span class="py">setAppName</span><span class="o">(</span><span class="s">"NetworkWordCount"</span><span class="o">)</span>
<span class="mi">47</span>     <span class="k">val</span> <span class="nv">ssc</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">StreamingContext</span><span class="o">(</span><span class="n">sparkConf</span><span class="o">,</span> <span class="nc">Seconds</span><span class="o">(</span><span class="mi">1</span><span class="o">))</span>
<span class="mi">48</span>
<span class="mi">49</span>     <span class="c1">// Create a socket stream on target ip:port and count the
</span><span class="mi">50</span>     <span class="c1">// words in input stream of \n delimited text (eg. generated by 'nc')
</span><span class="mi">51</span>     <span class="c1">// Note that no duplication in storage level only for running locally.
</span><span class="mi">52</span>     <span class="c1">// Replication necessary in distributed scenario for fault tolerance.
</span><span class="mi">53</span>     <span class="k">val</span> <span class="nv">lines</span> <span class="k">=</span> <span class="nv">ssc</span><span class="o">.</span><span class="py">socketTextStream</span><span class="o">(</span><span class="nf">args</span><span class="o">(</span><span class="mi">0</span><span class="o">),</span> <span class="nf">args</span><span class="o">(</span><span class="mi">1</span><span class="o">).</span><span class="py">toInt</span><span class="o">,</span> <span class="nv">StorageLevel</span><span class="o">.</span><span class="py">MEMORY_AND_DISK_SER</span><span class="o">)</span>
<span class="mi">54</span>     <span class="k">val</span> <span class="nv">words</span> <span class="k">=</span> <span class="nv">lines</span><span class="o">.</span><span class="py">flatMap</span><span class="o">(</span><span class="nv">_</span><span class="o">.</span><span class="py">split</span><span class="o">(</span><span class="s">" "</span><span class="o">))</span>
<span class="mi">55</span>     <span class="k">val</span> <span class="nv">wordCounts</span> <span class="k">=</span> <span class="nv">words</span><span class="o">.</span><span class="py">map</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">x</span><span class="o">,</span> <span class="mi">1</span><span class="o">)).</span><span class="py">reduceByKey</span><span class="o">(</span><span class="k">_</span> <span class="o">+</span> <span class="k">_</span><span class="o">)</span>
<span class="mi">56</span>     <span class="nv">wordCounts</span><span class="o">.</span><span class="py">print</span><span class="o">()</span>
<span class="mi">57</span>     <span class="nv">ssc</span><span class="o">.</span><span class="py">start</span><span class="o">()</span>
<span class="mi">58</span>     <span class="nv">ssc</span><span class="o">.</span><span class="py">awaitTermination</span><span class="o">()</span>
<span class="mi">59</span>   <span class="o">}</span>
<span class="mi">60</span> <span class="o">}</span>
<span class="mi">61</span> <span class="c1">// scalastyle:on println
</span></code></pre></div></div>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span><span class="kt">hadoop@NN01</span> <span class="kt">~</span><span class="o">]</span><span class="n">$</span> <span class="n">nc</span> <span class="o">-</span><span class="n">lp</span> <span class="mi">9999</span>
<span class="nc">Hello</span> <span class="n">world</span>
<span class="nc">This</span> <span class="n">is</span> <span class="n">david</span>
<span class="n">stop</span> <span class="n">now</span>
</code></pre></div></div>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span><span class="kt">hadoop@NN01</span> <span class="kt">spark</span><span class="o">]</span><span class="n">$</span> <span class="o">./</span><span class="n">bin</span><span class="o">/</span><span class="n">run</span><span class="o">-</span><span class="n">example</span> <span class="nv">streaming</span><span class="o">.</span><span class="py">NetworkWordCount</span> <span class="nv">NN01</span><span class="o">.</span><span class="py">HadoopVM</span> <span class="mi">9999</span>
<span class="o">-------------------------------------------</span>
<span class="nc">Time</span><span class="k">:</span> <span class="err">1473325234000</span> <span class="kt">ms</span>
<span class="o">-------------------------------------------</span>
<span class="o">(</span><span class="n">is</span><span class="o">,</span><span class="mi">1</span><span class="o">)</span>
<span class="o">(</span><span class="nc">Hello</span><span class="o">,</span><span class="mi">1</span><span class="o">)</span>
<span class="o">(</span><span class="nc">This</span><span class="o">,</span><span class="mi">1</span><span class="o">)</span>
<span class="o">(</span><span class="n">david</span><span class="o">,</span><span class="mi">1</span><span class="o">)</span>
<span class="o">(</span><span class="n">world</span><span class="o">,</span><span class="mi">1</span><span class="o">)</span>
<span class="o">...</span>
<span class="nc">Time</span><span class="k">:</span> <span class="err">1473325243000</span> <span class="kt">ms</span>
<span class="o">-------------------------------------------</span>
<span class="o">(</span><span class="n">stop</span><span class="o">,</span><span class="mi">1</span><span class="o">)</span>
<span class="o">(</span><span class="n">now</span><span class="o">,</span><span class="mi">1</span><span class="o">)</span>
</code></pre></div></div>

<h3 id="3-basic-concepts">3 Basic Concepts</h3>

<h4 id="31-linking">3.1 Linking</h4>

<p>Similar to Spark, Spark Streaming is available through Maven Central. To write your own Spark Streaming program, you will have to add the following dependency to your SBT or Maven project.</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">#</span> <span class="n">maven</span>
<span class="o">&lt;</span><span class="n">dependency</span><span class="o">&gt;</span>
    <span class="o">&lt;</span><span class="n">groupId</span><span class="o">&gt;</span><span class="nv">org</span><span class="o">.</span><span class="py">apache</span><span class="o">.</span><span class="py">spark</span><span class="o">&lt;/</span><span class="n">groupId</span><span class="o">&gt;</span>
    <span class="o">&lt;</span><span class="n">artifactId</span><span class="o">&gt;</span><span class="n">spark</span><span class="o">-</span><span class="n">streaming_2</span><span class="o">.</span><span class="mi">11</span><span class="o">&lt;/</span><span class="n">artifactId</span><span class="o">&gt;</span>
    <span class="o">&lt;</span><span class="n">version</span><span class="o">&gt;</span><span class="mf">2.0</span><span class="o">.</span><span class="mi">0</span><span class="o">&lt;/</span><span class="n">version</span><span class="o">&gt;</span>
<span class="o">&lt;/</span><span class="n">dependency</span><span class="o">&gt;</span>

<span class="k">#</span> <span class="nc">SBT</span>
<span class="n">libraryDependencies</span> <span class="o">+=</span> <span class="s">"org.apache.spark"</span> <span class="o">%</span> <span class="s">"spark-streaming_2.11"</span> <span class="o">%</span> <span class="s">"2.0.0"</span>
</code></pre></div></div>

<p>For ingesting data from sources like Kafka, Flume, and Kinesis that are not present in the Spark Streaming core API, you will have to add the corresponding artifact spark-streaming-xyz_2.11 to the dependencies. For example, some of the common ones are as follows.</p>

<table>
  <tbody>
    <tr>
      <td><strong>Source</strong></td>
      <td><strong>Artifact</strong></td>
    </tr>
    <tr>
      <td>Kafka</td>
      <td>spark-streaming-kafka-0-8_2.11</td>
    </tr>
    <tr>
      <td>Flume</td>
      <td>spark-streaming-flume_2.11</td>
    </tr>
    <tr>
      <td>Kinesis</td>
      <td>spark-streaming-kinesis-asl_2.11 [Amazon Software License]</td>
    </tr>
  </tbody>
</table>

<p>For an up-to-date list, please refer to the <a href="http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22org.apache.spark%22%20AND%20v%3A%222.0.0%22">Maven repository</a> for the full list of supported sources and artifacts.</p>

<h4 id="32-initializing-streamingcontext">3.2 Initializing <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.StreamingContext">StreamingContext</a></h4>

<p>To initialize a Spark Streaming program, a <strong>StreamingContext object(StreamingContext对象)</strong> has to be created which is the main entry point of all Spark Streaming functionality.</p>

<p><strong>scala</strong></p>

<p>A StreamingContext object can be created from a <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkConf">SparkConf object</a>.</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">org.apache.spark._</span>
<span class="k">import</span> <span class="nn">org.apache.spark.streaming._</span>

<span class="k">val</span> <span class="nv">conf</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SparkConf</span><span class="o">().</span><span class="py">setAppName</span><span class="o">(</span><span class="n">appName</span><span class="o">).</span><span class="py">setMaster</span><span class="o">(</span><span class="n">master</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">ssc</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">StreamingContext</span><span class="o">(</span><span class="n">conf</span><span class="o">,</span> <span class="nc">Seconds</span><span class="o">(</span><span class="mi">1</span><span class="o">))</span>
</code></pre></div></div>

<p><strong>python</strong></p>

<p>A StreamingContext object can be created from a <a href="http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext">SparkContext object</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">pyspark</span> <span class="kn">import</span> <span class="n">SparkContext</span>
<span class="kn">from</span> <span class="nn">pyspark.streaming</span> <span class="kn">import</span> <span class="n">StreamingContext</span>

<span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="p">(</span><span class="n">master</span><span class="p">,</span> <span class="n">appName</span><span class="p">)</span>
<span class="n">ssc</span> <span class="o">=</span> <span class="n">StreamingContext</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>The <strong>appName</strong> parameter is a name for your application to show on the cluster UI. master is a <a href="http://spark.apache.org/docs/latest/submitting-applications.html#master-urls">Spark, Mesos or YARN cluster URL</a>, or a special <strong>“local[*]”</strong> string to run in local mode. In practice, when running on a cluster, you will not want to hardcode master in the program, but rather <a href="http://spark.apache.org/docs/latest/submitting-applications.html">launch the application with spark-submit</a> and receive it there. However, for local testing and unit tests, you can pass “local[*]” to run Spark Streaming in-process (detects the number of cores in the local system). Note that this internally creates a SparkContext (starting point of all Spark functionality) which can be accessed as ssc.sparkContext.</p>

<ul>
  <li>The batch interval must be set based on the latency requirements of your application and available cluster resources. See the <a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#setting-the-right-batch-interval">Performance Tuning</a> section for more details.</li>
  <li>A StreamingContext object can also be created from an existing SparkContext object.</li>
  <li>
    <p>After a context is defined, you have to do the following.</p>
  </li>
  <li>1.Define the input sources by creating input DStreams.</li>
  <li>2.Define the streaming computations by applying transformation and output operations to DStreams.</li>
  <li>3.Start receiving data and processing it using streamingContext.start().</li>
  <li>4.Wait for the processing to be stopped (manually or due to any error) using streamingContext.awaitTermination().</li>
  <li>5.The processing can be manually stopped using streamingContext.stop().</li>
</ul>

<p><strong>Points to remember</strong></p>

<ul>
  <li>Once a context has been started, no new streaming computations can be set up or added to it.</li>
  <li>Once a context has been stopped, it cannot be restarted.</li>
  <li>Only one StreamingContext can be active in a JVM at the same time.</li>
  <li>stop() on StreamingContext also stops the SparkContext. To stop only the StreamingContext, set the optional parameter of stop() called stopSparkContext to false.</li>
  <li>A SparkContext can be re-used to create multiple StreamingContexts, as long as the previous StreamingContext is stopped (without stopping the SparkContext) before the next StreamingContext is created.</li>
</ul>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">org.apache.spark.streaming._</span>

<span class="k">val</span> <span class="nv">sc</span> <span class="k">=</span> <span class="o">...</span>                <span class="c1">// existing SparkContext
</span><span class="k">val</span> <span class="nv">ssc</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">StreamingContext</span><span class="o">(</span><span class="n">sc</span><span class="o">,</span> <span class="nc">Seconds</span><span class="o">(</span><span class="mi">1</span><span class="o">))</span>
</code></pre></div></div>

<h4 id="33-discretized-streamsdstreams">3.3 Discretized Streams(DStreams)</h4>

<p><strong>Discretized Stream</strong> or <strong>DStream</strong> is the basic abstraction provided by Spark Streaming. It represents a continuous stream of data, either the input data stream received from source, or the processed data stream generated by transforming the input stream. <u>Internally, a DStream is represented by a continuous series of RDDs</u>, which is Spark’s abstraction of an immutable, distributed dataset (see <a href="http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds">Spark Programming Guide</a> for more details). Each RDD in a DStream contains data from a certain interval, as shown in the following figure.</p>

<p><img src="/images/hadoop/streaming-dstream.png" alt="" /></p>

<p>Any operation applied on a DStream translates to operations on the underlying RDDs. For example, in the <a href="/hadoop/2016/09/08/Streaming_lesson1/#a-quick-example">earlier example</a> of converting a stream of lines to words, the <strong>flatMap</strong> operation is applied on each RDD in the lines DStream to generate the RDDs of the words DStream. This is shown in the following figure.</p>

<p><img src="/images/hadoop/streaming-dstream-ops.png" alt="" />
These underlying RDD transformations are computed by the Spark engine. The DStream operations hide most of these details and provide the developer with a higher-level API for convenience. These operations are discussed in detail in later sections.</p>

<h4 id="34-input-dstreams-and-receivers">3.4 Input Dstreams and Receivers</h4>

<p>Input DStreams are DStreams representing the stream of input data received from streaming sources. In the <a href="/hadoop/2016/09/08/Streaming_lesson1/#a-quick-example">quick example</a>, lines was an input DStream as it represented the stream of data received from the netcat server. Every input DStream (except file stream, discussed later in this section) is associated with a Receiver (<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.receiver.Receiver">Scala doc</a>, <a href="http://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/receiver/Receiver.html">Java doc</a>) object which receives the data from a source and stores it in Spark’s memory for processing.一个input DStream是一个特殊的DStream，将Spark Streaming连接到一个外部数据源来读取数据。</p>

<p>Spark Streaming provides two categories of built-in streaming sources.</p>

<ul>
  <li><strong>Basic sources</strong>: Sources directly available in the StreamingContext API. Examples: file systems, and socket connections.</li>
  <li><strong>Advanced sources</strong>: Sources like Kafka, Flume, Kinesis, etc. are available through extra utility classes. These require linking against extra dependencies as discussed in the linking section.</li>
</ul>

<p><strong>File Streams</strong></p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">#</span> <span class="n">scala</span>
<span class="nv">streamingContext</span><span class="o">.</span><span class="py">fileStream</span><span class="o">[</span><span class="kt">KeyClass</span>, <span class="kt">ValueClass</span>, <span class="kt">InputFormatClass</span><span class="o">](</span><span class="n">dataDirectory</span><span class="o">)</span>
<span class="k">#</span> <span class="n">python</span>
<span class="nv">streamingContext</span><span class="o">.</span><span class="py">textFileStream</span><span class="o">(</span><span class="n">dataDirectory</span><span class="o">)</span>
</code></pre></div></div>

<p><strong>Advanced Sources</strong></p>

<ul>
  <li>Kafka: Spark Streaming 2.0.0 is compatible with Kafka 0.8.2.1. See the <a href="http://spark.apache.org/docs/latest/streaming-kafka-integration.html">Kafka Integration Guide</a> for more details.</li>
  <li>Flume: Spark Streaming 2.0.0 is compatible with Flume 1.6.0. See the <a href="http://spark.apache.org/docs/latest/streaming-flume-integration.html">Flume Integration Guide</a> for more details.</li>
  <li>Kinesis: Spark Streaming 2.0.0 is compatible with Kinesis Client Library 1.2.1. See the <a href="http://spark.apache.org/docs/latest/streaming-kinesis-integration.html">Kinesis Integration Guide</a> for more details.</li>
</ul>

<h4 id="35-transformations-on-dstreams">3.5 Transformations on DStreams</h4>

<p>Similar to that of RDDs, transformations allow the data from the input DStream to be modified. DStreams support many of the transformations available on normal Spark RDD’s. Some of the common ones are as follows.</p>

<table>
  <tbody>
    <tr>
      <td><strong>Transformation</strong></td>
      <td><strong>Meaning</strong></td>
    </tr>
    <tr>
      <td>map(func)</td>
      <td>Return a new DStream by passing each element of the source DStream through a function func.</td>
    </tr>
    <tr>
      <td>flatMap(func)</td>
      <td>Similar to map, but each input item can be mapped to 0 or more output items.</td>
    </tr>
    <tr>
      <td>filter(func)</td>
      <td>Return a new DStream by selecting only the records of the source DStream on which func returns true.</td>
    </tr>
    <tr>
      <td>repartition(numPartitions)</td>
      <td>Changes the level of parallelism in this DStream by creating more or fewer partitions.</td>
    </tr>
    <tr>
      <td>union(otherStream)</td>
      <td>Return a new DStream that contains the union of the elements in the source DStream and otherDStream.</td>
    </tr>
    <tr>
      <td>count()</td>
      <td>Return a new DStream of single-element RDDs by counting the number of elements in each RDD of the source DStream.</td>
    </tr>
    <tr>
      <td>reduce(func)</td>
      <td>Return a new DStream of single-element RDDs by aggregating the elements in each RDD of the source DStream using a function func (which takes two arguments and returns one). The function should be associative and commutative so that it can be computed in parallel.</td>
    </tr>
    <tr>
      <td>countByValue()</td>
      <td>When called on a DStream of elements of type K, return a new DStream of (K, Long) pairs where the value of each key is its frequency in each RDD of the source DStream.</td>
    </tr>
    <tr>
      <td>reduceByKey(func, [numTasks])</td>
      <td>When called on a DStream of (K, V) pairs, return a new DStream of (K, V) pairs where the values for each key are aggregated using the given reduce function. <strong>Note</strong>: By default, this uses Spark’s default number of parallel tasks (2 for local mode, and in cluster mode the number is determined by the config property spark.default.parallelism) to do the grouping. You can pass an optional numTasks argument to set a different number of tasks.</td>
    </tr>
    <tr>
      <td>join(otherStream, [numTasks])</td>
      <td>When called on two DStreams of (K, V) and (K, W) pairs, return a new DStream of (K, (V, W)) pairs with all pairs of elements for each key.</td>
    </tr>
    <tr>
      <td>cogroup(otherStream, [numTasks])</td>
      <td>When called on a DStream of (K, V) and (K, W) pairs, return a new DStream of (K, Seq[V], Seq[W]) tuples.</td>
    </tr>
    <tr>
      <td><strong>transform(func)</strong></td>
      <td>Return a new DStream by applying a RDD-to-RDD function to every RDD of the source DStream. This can be used to do arbitrary RDD operations on the DStream.</td>
    </tr>
    <tr>
      <td><strong>updateStateByKey(func)</strong></td>
      <td>Return a new “state” DStream where the state for each key is updated by applying the given function on the previous state of the key and the new values for the key. This can be used to maintain arbitrary state data for each key.</td>
    </tr>
  </tbody>
</table>

<p><strong>UpdateStateByKey Operation</strong></p>

<p>The updateStateByKey operation allows you to maintain arbitrary state while continuously updating it with new information. To use this, you will have to do two steps.该 updateStateByKey 操作可以让你保持任意状态，同时不断有新的信息进行更新。</p>

<ul>
  <li>1.<strong>Define the state</strong> - The state can be an arbitrary data type.</li>
  <li>2.<strong>Define the state update function</strong> - Specify with a function how to update the state using the previous state and the new values from an input stream.</li>
</ul>

<p>In every batch, Spark will apply the state update function for all existing keys, regardless of whether they have new data in a batch or not. If the update function returns None then the key-value pair will be eliminated.</p>

<p>Let’s illustrate this with an example. Say you want to maintain a running count of each word seen in a text data stream. Here, the running count is the state and it is an integer. We define the update function as:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">updateFunction</span><span class="o">(</span><span class="n">newValues</span><span class="k">:</span> <span class="kt">Seq</span><span class="o">[</span><span class="kt">Int</span><span class="o">],</span> <span class="n">runningCount</span><span class="k">:</span> <span class="kt">Option</span><span class="o">[</span><span class="kt">Int</span><span class="o">])</span><span class="k">:</span> <span class="kt">Option</span><span class="o">[</span><span class="kt">Int</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
    <span class="k">val</span> <span class="nv">newCount</span> <span class="k">=</span> <span class="o">...</span>  <span class="c1">// add the new values with the previous running count to get the new count
</span>    <span class="nc">Some</span><span class="o">(</span><span class="n">newCount</span><span class="o">)</span>
<span class="o">}</span>
<span class="k">#</span> <span class="nc">This</span> <span class="n">is</span> <span class="n">applied</span> <span class="n">on</span> <span class="n">a</span> <span class="nc">DStream</span> <span class="n">containing</span> <span class="nf">words</span> <span class="o">(</span><span class="n">say</span><span class="o">,</span> <span class="n">the</span> <span class="n">pairs</span> <span class="nc">DStream</span> <span class="nf">containing</span> <span class="o">(</span><span class="n">word</span><span class="o">,</span> <span class="mi">1</span><span class="o">)</span> <span class="n">pairs</span> <span class="n">in</span> <span class="n">the</span> <span class="n">earlier</span> <span class="n">example</span><span class="o">).</span>

<span class="k">val</span> <span class="nv">runningCounts</span> <span class="k">=</span> <span class="nv">pairs</span><span class="o">.</span><span class="py">updateStateByKey</span><span class="o">[</span><span class="kt">Int</span><span class="o">](</span><span class="n">updateFunction</span> <span class="k">_</span><span class="o">)</span>
</code></pre></div></div>

<pre><code class="language-Python">def updateFunction(newValues, runningCount):
    if runningCount is None:
       runningCount = 0
    return sum(newValues, runningCount)  # add the new values with the previous running count to get the new count
# This is applied on a DStream containing words (say, the pairs DStream containing (word, 1) pairs in the earlier example).

runningCounts = pairs.updateStateByKey(updateFunction)
</code></pre>

<p>The update function will be called for each word, with newValues having a sequence of 1’s (from the (word, 1) pairs) and the runningCount having the previous count. For the complete Python code, take a look at the example <a href="https://github.com/apache/spark/blob/master/examples/src/main/python/streaming/stateful_network_wordcount.py">stateful_network_wordcount.py</a>.</p>

<p>Note that using updateStateByKey requires the checkpoint directory to be configured, which is discussed in detail in the <a href="/hadoop/2016/09/08/Streaming_lesson1/#checkpointing">checkpointing section</a>.</p>

<p><strong>Transform Operation</strong></p>

<p>The transform operation (along with its variations like transformWith) allows arbitrary RDD-to-RDD functions to be applied on a DStream. It can be used to apply any RDD operation that is not exposed in the DStream API. For example, the functionality of joining every batch in a data stream with another dataset is not directly exposed in the DStream API. However, you can easily use transform to do this. This enables very powerful possibilities. For example, one can do real-time data cleaning by joining the input data stream with precomputed spam information (maybe generated with Spark as well) and then filtering based on it.
该transform操作（转换操作）连同其其类似的 transformWith操作允许DStream 上应用任意RDD-to-RDD函数。它可以被应用于未在 DStream API 中暴露任何的RDD操作。例如，在每批次的数据流与另一数据集的连接功能不直接暴露在DStream API 中，但可以轻松地使用transform操作来做到这一点，这使得DStream的功能非常强大。例如，你可以通过连接预先计算的垃圾邮件信息的输入数据流（可能也有Spark生成的），然后基于此做实时数据清理的筛选，如下面官方提供的伪代码所示。事实上，也可以在transform方法中使用机器学习和图形计算的算法。</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">val</span> <span class="nv">spamInfoRDD</span> <span class="k">=</span> <span class="nv">ssc</span><span class="o">.</span><span class="py">sparkContext</span><span class="o">.</span><span class="py">newAPIHadoopRDD</span><span class="o">(...)</span> <span class="c1">// RDD containing spam information
</span>
<span class="k">val</span> <span class="nv">cleanedDStream</span> <span class="k">=</span> <span class="nv">wordCounts</span><span class="o">.</span><span class="py">transform</span><span class="o">(</span><span class="n">rdd</span> <span class="k">=&gt;</span> <span class="o">{</span>
  <span class="nv">rdd</span><span class="o">.</span><span class="py">join</span><span class="o">(</span><span class="n">spamInfoRDD</span><span class="o">).</span><span class="py">filter</span><span class="o">(...)</span> <span class="c1">// join data stream with spam information to do data cleaning
</span>  <span class="o">...</span>
<span class="o">})</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">spamInfoRDD</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">pickleFile</span><span class="p">(</span><span class="o">...</span><span class="p">)</span> <span class="c1"># RDD containing spam information
</span>
<span class="c1"># join data stream with spam information to do data cleaning
</span><span class="n">cleanedDStream</span> <span class="o">=</span> <span class="n">wordCounts</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="k">lambda</span> <span class="n">rdd</span><span class="p">:</span> <span class="n">rdd</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">spamInfoRDD</span><span class="p">)</span><span class="o">.</span><span class="nb">filter</span><span class="p">(</span><span class="o">...</span><span class="p">))</span>
</code></pre></div></div>

<p><strong>Window Operations</strong></p>

<p>Spark Streaming also provides windowed computations, which allow you to apply transformations over a sliding window of data(通过滑动窗口对数据进行转换). The following figure illustrates this sliding window.</p>

<p><img src="/images/hadoop/streaming-dstream-window.png" alt="" /></p>

<p>As shown in the figure, every time the window slides over a source DStream, the source RDDs that fall within the window are combined and operated upon to produce the RDDs of the windowed DStream. In this specific case, the operation is applied over the last 3 time units of data, and slides by 2 time units. This shows that any window operation needs to specify two parameters.</p>

<ul>
  <li><strong>window length</strong> - The duration of the window (3 in the figure).</li>
  <li><strong>sliding interval</strong> - The interval at which the window operation is performed (2 in the figure).</li>
</ul>

<p>在Spark Streaming中，数据处理是按批进行的，而数据采集是逐条进行的，因此在Spark Streaming中会先设置好批处理间隔（batch duration），当超过批处理间隔的时候就会把采集到的数据汇总起来成为一批数据交给系统去处理。</p>

<p>对于窗口操作而言，在其窗口内部会有N个批处理数据，批处理数据的大小由窗口间隔（window duration）决定，而窗口间隔指的就是窗口的持续时间，在窗口操作中，只有窗口的长度满足了才会触发批数据的处理。除了窗口的长度，窗口操作还有另一个重要的参数就是滑动间隔（slide duration），它指的是经过多长时间窗口滑动一次形成新的窗口，滑动窗口默认情况下和批次间隔的相同，而窗口间隔一般设置的要比它们两个大。在这里必须注意的一点是<strong>滑动间隔和窗口间隔的大小一定得设置为批处理间隔的整数倍</strong>。</p>

<p>如批处理间隔示意图所示，批处理间隔是1个时间单位，窗口间隔是3个时间单位，滑动间隔是2个时间单位。对于初始的窗口time 1-time 3，只有窗口间隔满足了才触发数据的处理。这里需要注意的一点是，初始的窗口有可能流入的数据没有撑满，但是随着时间的推进，窗口最终会被撑满。当每个2个时间单位，窗口滑动一次后，会有新的数据流入窗口，这时窗口会移去最早的两个时间单位的数据，而与最新的两个时间单位的数据进行汇总形成新的窗口（time3-time5）。</p>

<p>对于窗口操作，<strong>批处理间隔、窗口间隔和滑动间隔</strong>是非常重要的三个时间概念，是理解窗口操作的关键所在。</p>

<p>Let’s illustrate the window operations with an example. Say, you want to extend the earlier example by generating word counts over the last 30 seconds of data, every 10 seconds. To do this, we have to apply the reduceByKey operation on the pairs DStream of (word, 1) pairs over the last 30 seconds of data. This is done using the operation reduceByKeyAndWindow.</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// Reduce last 30 seconds of data, every 10 seconds
</span><span class="k">val</span> <span class="nv">windowedWordCounts</span> <span class="k">=</span> <span class="nv">pairs</span><span class="o">.</span><span class="py">reduceByKeyAndWindow</span><span class="o">((</span><span class="n">a</span><span class="k">:</span><span class="kt">Int</span><span class="o">,</span><span class="n">b</span><span class="k">:</span><span class="kt">Int</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="o">),</span> <span class="nc">Seconds</span><span class="o">(</span><span class="mi">30</span><span class="o">),</span> <span class="nc">Seconds</span><span class="o">(</span><span class="mi">10</span><span class="o">))</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Reduce last 30 seconds of data, every 10 seconds
</span><span class="n">windowedWordCounts</span> <span class="o">=</span> <span class="n">pairs</span><span class="o">.</span><span class="n">reduceByKeyAndWindow</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span> <span class="o">-</span> <span class="n">y</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</code></pre></div></div>

<p>Some of the common window operations are as follows. All of these operations take the said two parameters - windowLength and slideInterval.</p>

<table>
  <tbody>
    <tr>
      <td>Transformation</td>
      <td>Meaning</td>
    </tr>
    <tr>
      <td>window(windowLength, slideInterval)</td>
      <td>Return a new DStream which is computed based on windowed batches of the source DStream.</td>
    </tr>
    <tr>
      <td>countByWindow(windowLength, slideInterval)</td>
      <td>Return a sliding window count of elements in the stream.</td>
    </tr>
    <tr>
      <td>reduceByWindow(func, windowLength, slideInterval)</td>
      <td>Return a new single-element stream, created by aggregating elements in the stream over a sliding interval using func. The function should be associative and commutative so that it can be computed correctly in parallel.</td>
    </tr>
    <tr>
      <td>reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks])</td>
      <td>When called on a DStream of (K, V) pairs, returns a new DStream of (K, V) pairs where the values for each key are aggregated using the given reduce function func over batches in a sliding window. Note: By default, this uses Spark’s default number of parallel tasks (2 for local mode, and in cluster mode the number is determined by the config property spark.default.parallelism) to do the grouping. You can pass an optional numTasks argument to set a different number of tasks.</td>
    </tr>
    <tr>
      <td>reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks])</td>
      <td>A more efficient version of the above reduceByKeyAndWindow() where the reduce value of each window is calculated incrementally using the reduce values of the previous window. This is done by reducing the new data that enters the sliding window, and “inverse reducing” the old data that leaves the window. An example would be that of “adding” and “subtracting” counts of keys as the window slides. However, it is applicable only to “invertible reduce functions”, that is, those reduce functions which have a corresponding “inverse reduce” function (taken as parameter invFunc). Like in reduceByKeyAndWindow, the number of reduce tasks is configurable through an optional argument. Note that checkpointing must be enabled for using this operation.</td>
    </tr>
    <tr>
      <td>countByValueAndWindow(windowLength, slideInterval, [numTasks])</td>
      <td>When called on a DStream of (K, V) pairs, returns a new DStream of (K, Long) pairs where the value of each key is its frequency within a sliding window. Like in reduceByKeyAndWindow, the number of reduce tasks is configurable through an optional argument.</td>
    </tr>
  </tbody>
</table>

<p><strong>Join Operations</strong></p>

<p><em>Stream-stream joins</em></p>

<p>Streams can be very easily joined with other streams.</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">val</span> <span class="nv">stream1</span><span class="k">:</span> <span class="kt">DStream</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>
<span class="k">val</span> <span class="nv">stream2</span><span class="k">:</span> <span class="kt">DStream</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>
<span class="k">val</span> <span class="nv">joinedStream</span> <span class="k">=</span> <span class="nv">stream1</span><span class="o">.</span><span class="py">join</span><span class="o">(</span><span class="n">stream2</span><span class="o">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">stream1</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">stream2</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">joinedStream</span> <span class="o">=</span> <span class="n">stream1</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">stream2</span><span class="p">)</span>
</code></pre></div></div>

<p>Here, in each batch interval, the RDD generated by stream1 will be joined with the RDD generated by stream2. You can also do leftOuterJoin, rightOuterJoin, fullOuterJoin. Furthermore, it is often very useful to do joins over windows of the streams. That is pretty easy as well.</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">val</span> <span class="nv">windowedStream1</span> <span class="k">=</span> <span class="nv">stream1</span><span class="o">.</span><span class="py">window</span><span class="o">(</span><span class="nc">Seconds</span><span class="o">(</span><span class="mi">20</span><span class="o">))</span>
<span class="k">val</span> <span class="nv">windowedStream2</span> <span class="k">=</span> <span class="nv">stream2</span><span class="o">.</span><span class="py">window</span><span class="o">(</span><span class="nc">Minutes</span><span class="o">(</span><span class="mi">1</span><span class="o">))</span>
<span class="k">val</span> <span class="nv">joinedStream</span> <span class="k">=</span> <span class="nv">windowedStream1</span><span class="o">.</span><span class="py">join</span><span class="o">(</span><span class="n">windowedStream2</span><span class="o">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">windowedStream1</span> <span class="o">=</span> <span class="n">stream1</span><span class="o">.</span><span class="n">window</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
<span class="n">windowedStream2</span> <span class="o">=</span> <span class="n">stream2</span><span class="o">.</span><span class="n">window</span><span class="p">(</span><span class="mi">60</span><span class="p">)</span>
<span class="n">joinedStream</span> <span class="o">=</span> <span class="n">windowedStream1</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">windowedStream2</span><span class="p">)</span>
</code></pre></div></div>

<p><em>Stream-dataset joins</em></p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">val</span> <span class="nv">dataset</span><span class="k">:</span> <span class="kt">RDD</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>
<span class="k">val</span> <span class="nv">windowedStream</span> <span class="k">=</span> <span class="nv">stream</span><span class="o">.</span><span class="py">window</span><span class="o">(</span><span class="nc">Seconds</span><span class="o">(</span><span class="mi">20</span><span class="o">))...</span>
<span class="k">val</span> <span class="nv">joinedStream</span> <span class="k">=</span> <span class="nv">windowedStream</span><span class="o">.</span><span class="py">transform</span> <span class="o">{</span> <span class="n">rdd</span> <span class="k">=&gt;</span> <span class="nv">rdd</span><span class="o">.</span><span class="py">join</span><span class="o">(</span><span class="n">dataset</span><span class="o">)</span> <span class="o">}</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dataset</span> <span class="o">=</span> <span class="o">...</span> <span class="c1"># some RDD
</span><span class="n">windowedStream</span> <span class="o">=</span> <span class="n">stream</span><span class="o">.</span><span class="n">window</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
<span class="n">joinedStream</span> <span class="o">=</span> <span class="n">windowedStream</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="k">lambda</span> <span class="n">rdd</span><span class="p">:</span> <span class="n">rdd</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dataset</span><span class="p">))</span>
</code></pre></div></div>

<p><strong>Output Operations on DStreams</strong></p>

<p>Spark Streaming允许DStream的数据被输出到外部系统，如数据库或文件系统。由于输出操作实际上使transformation操作后的数据可以通过外部系统被使用，同时输出操作触发所有DStream的transformation操作的实际执行（类似于RDD action）。以下表列出了目前主要的输出操作：</p>

<table>
  <tbody>
    <tr>
      <td>Output Operation</td>
      <td>Meaning</td>
    </tr>
    <tr>
      <td>print()</td>
      <td>Prints the first ten elements of every batch of data in a DStream on the driver node running the streaming application. This is useful for development and debugging.</td>
    </tr>
    <tr>
      <td>saveAsTextFiles(prefix, [suffix])</td>
      <td>Save this DStream’s contents as text files. The file name at each batch interval is generated based on prefix and suffix: “prefix-TIME_IN_MS[.suffix]”.</td>
    </tr>
    <tr>
      <td>saveAsObjectFiles(prefix, [suffix])</td>
      <td>Save this DStream’s contents as SequenceFiles of serialized Java objects. The file name at each batch interval is generated based on prefix and suffix: “prefix-TIME_IN_MS[.suffix]”.</td>
    </tr>
    <tr>
      <td>saveAsHadoopFiles(prefix, [suffix])</td>
      <td>Save this DStream’s contents as Hadoop files. The file name at each batch interval is generated based on prefix and suffix: “prefix-TIME_IN_MS[.suffix]”.</td>
    </tr>
    <tr>
      <td>foreachRDD(func)</td>
      <td>The most generic output operator that applies a function, func, to each RDD generated from the stream. This function should push the data in each RDD to an external system, such as saving the RDD to files, or writing it over the network to a database. Note that the function func is executed in the driver process running the streaming application, and will usually have RDD actions in it that will force the computation of the streaming RDDs.</td>
    </tr>
  </tbody>
</table>

<p><strong>Design Patterns for using foreachRDD</strong></p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">dstream</span><span class="o">.</span><span class="py">foreachRDD</span> <span class="o">{</span> <span class="n">rdd</span> <span class="k">=&gt;</span>
  <span class="nv">rdd</span><span class="o">.</span><span class="py">foreachPartition</span> <span class="o">{</span> <span class="n">partitionOfRecords</span> <span class="k">=&gt;</span>
    <span class="c1">// ConnectionPool is a static, lazily initialized pool of connections
</span>    <span class="k">val</span> <span class="nv">connection</span> <span class="k">=</span> <span class="nv">ConnectionPool</span><span class="o">.</span><span class="py">getConnection</span><span class="o">()</span>
    <span class="nv">partitionOfRecords</span><span class="o">.</span><span class="py">foreach</span><span class="o">(</span><span class="n">record</span> <span class="k">=&gt;</span> <span class="nv">connection</span><span class="o">.</span><span class="py">send</span><span class="o">(</span><span class="n">record</span><span class="o">))</span>
    <span class="nv">ConnectionPool</span><span class="o">.</span><span class="py">returnConnection</span><span class="o">(</span><span class="n">connection</span><span class="o">)</span>  <span class="c1">// return to the pool for future reuse
</span>  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">sendPartition</span><span class="p">(</span><span class="nb">iter</span><span class="p">):</span>
    <span class="c1"># ConnectionPool is a static, lazily initialized pool of connections
</span>    <span class="n">connection</span> <span class="o">=</span> <span class="n">ConnectionPool</span><span class="o">.</span><span class="n">getConnection</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">record</span> <span class="ow">in</span> <span class="nb">iter</span><span class="p">:</span>
        <span class="n">connection</span><span class="o">.</span><span class="n">send</span><span class="p">(</span><span class="n">record</span><span class="p">)</span>
    <span class="c1"># return to the pool for future reuse
</span>    <span class="n">ConnectionPool</span><span class="o">.</span><span class="n">returnConnection</span><span class="p">(</span><span class="n">connection</span><span class="p">)</span>

<span class="n">dstream</span><span class="o">.</span><span class="n">foreachRDD</span><span class="p">(</span><span class="k">lambda</span> <span class="n">rdd</span><span class="p">:</span> <span class="n">rdd</span><span class="o">.</span><span class="n">foreachPartition</span><span class="p">(</span><span class="n">sendPartition</span><span class="p">))</span>
</code></pre></div></div>
<p>通常情况下，创建一个连接对象有时间和资源开销。因此，创建和销毁的每条记录的连接对象可能招致不必要的资源开销，并显著降低系统整体的吞吐量 。一个更好的解决方案是使用rdd.foreachPartition方法创建一个单独的连接对象，然后使用该连接对象输出的所有RDD分区中的数据到外部系统。</p>

<p>这缓解了创建多条记录连接的开销。最后，还可以进一步通过在多个RDDs/ batches上重用连接对象进行优化。一个保持连接对象的静态池可以重用在多个批处理的RDD上将其输出到外部系统，从而进一步降低了开销。</p>

<p>需要注意的是，在静态池中的连接应该按需延迟创建，这样可以更有效地把数据发送到外部系统。另外需要要注意的是：DStreams延迟执行的，就像RDD的操作是由actions触发一样。默认情况下，输出操作会按照它们在Streaming应用程序中定义的顺序一个个执行。</p>

<h4 id="36-accumulators-and-broadcast-variables">3.6 Accumulators and Broadcast Variables</h4>

<p>我们传递给Spark的函数，如map()，或者filter()的判断条件函数，能够利用定义在函数之外的变量，但是集群中的每一个task都会得到变量的一个副本，并且task在对变量进行的更新不会被返回给driver。而Spark的两种共享变量：累加器（accumulator）和广播变量（broadcast variable），在广播和结果聚合这两种常见类型的通信模式上放宽了这种限制。
<a href="http://blog.csdn.net/Camu7s/article/details/49367111">Spark 共享变量——累加器（accumulator）与广播变量（broadcast variable）</a></p>

<p><strong>Accumulators累加器</strong></p>

<p>使用累加器可以很简便地对各个worker返回给driver的值进行聚合。累加器最常见的用途之一就是对一个job执行期间发生的事件进行计数。例如，当我们统计输入文件信息时，有时需要统计空白行的数量。下面的程序描述了这个过程。</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">scala</span><span class="o">&gt;</span> <span class="k">val</span> <span class="nv">file</span> <span class="k">=</span> <span class="nv">sc</span><span class="o">.</span><span class="py">textFile</span><span class="o">(</span><span class="s">"README.md"</span><span class="o">)</span>
<span class="n">file</span><span class="k">:</span> <span class="kt">org.apache.spark.rdd.RDD</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="nv">README</span><span class="o">.</span><span class="py">md</span> <span class="nc">MapPart12</span><span class="err">]</span> <span class="n">at</span> <span class="n">textFile</span> <span class="n">at</span> <span class="o">&lt;</span><span class="n">console</span><span class="k">&gt;:</span><span class="mi">24</span>

<span class="n">scala</span><span class="o">&gt;</span> <span class="k">val</span> <span class="nv">blankLines</span> <span class="k">=</span> <span class="nv">sc</span><span class="o">.</span><span class="py">accumulator</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
<span class="n">warning</span><span class="k">:</span> <span class="kt">there</span> <span class="kt">were</span> <span class="kt">two</span> <span class="kt">deprecation</span> <span class="kt">warnings</span><span class="o">;</span> <span class="n">re</span><span class="o">-</span><span class="n">run</span> <span class="k">with</span> <span class="n">on</span> <span class="k">for</span> <span class="n">details</span>
<span class="n">blankLines</span><span class="k">:</span> <span class="kt">org.apache.spark.Accumulator</span><span class="o">[</span><span class="kt">Int</span><span class="o">]</span> <span class="k">=</span> <span class="mi">0</span>

<span class="n">scala</span><span class="o">&gt;</span> <span class="k">val</span> <span class="nv">callSigns</span> <span class="k">=</span> <span class="nv">file</span><span class="o">.</span><span class="py">flatMap</span><span class="o">(</span><span class="n">line</span> <span class="k">=&gt;</span> <span class="o">{</span>
     <span class="o">|</span> <span class="nf">if</span> <span class="o">(</span><span class="n">line</span> <span class="o">==</span> <span class="s">""</span><span class="o">)</span> <span class="o">{</span>
     <span class="o">|</span>   <span class="n">blankLines</span> <span class="o">+=</span> <span class="mi">1</span>
     <span class="o">|</span> <span class="o">}</span>
     <span class="o">|</span> <span class="nv">line</span><span class="o">.</span><span class="py">split</span><span class="o">(</span><span class="s">" "</span><span class="o">)</span>
     <span class="o">|</span> <span class="o">})</span>
<span class="n">callSigns</span><span class="k">:</span> <span class="kt">org.apache.spark.rdd.RDD</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="nc">MapPartitionsRDD</span><span class="o">[</span><span class="err">13</span><span class="o">]</span> <span class="n">at</span> <span class="n">flatMap</span> <span class="n">at</span> <span class="o">&lt;</span><span class="n">console</span><span class="k">&gt;:</span><span class="mi">28</span>

<span class="n">scala</span><span class="o">&gt;</span> <span class="nv">callSigns</span><span class="o">.</span><span class="py">saveAsTextFile</span><span class="o">(</span><span class="s">"Output.txt"</span><span class="o">)</span>
<span class="o">[</span><span class="kt">Stage</span> <span class="err">0</span><span class="kt">:&gt;</span>                                                <span class="o">[</span><span class="kt">Stage</span> <span class="err">0</span><span class="kt">:&gt;</span>                                                <span class="o">[</span><span class="kt">Stage</span> <span class="err">0</span><span class="kt">:=============================&gt;</span>
<span class="kt">scala&gt;</span> <span class="kt">println</span><span class="o">(</span><span class="err">"</span><span class="kt">Blank</span> <span class="kt">Lines:</span> <span class="err">"</span> <span class="kt">+</span> <span class="kt">blankLines.value</span><span class="o">)</span>
<span class="kt">Blank</span> <span class="kt">Lines:</span> <span class="err">2</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="nb">file</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="s">"README.md"</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># create Accumulator[Int] initialized to 0
</span><span class="o">...</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">blankLines</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">accumulator</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">def</span> <span class="nf">extractCallSigns</span><span class="p">(</span><span class="n">line</span><span class="p">):</span>
<span class="o">...</span>     <span class="k">global</span> <span class="n">blankLines</span> <span class="c1"># make the global variable
</span><span class="o">...</span>     <span class="k">if</span> <span class="p">(</span><span class="n">line</span> <span class="o">==</span> <span class="s">""</span><span class="p">):</span>
<span class="o">...</span>         <span class="n">blankLines</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="o">...</span>     <span class="k">return</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">" "</span><span class="p">)</span>
<span class="o">...</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">callSigns</span> <span class="o">=</span> <span class="nb">file</span><span class="o">.</span><span class="n">flatMap</span><span class="p">(</span><span class="n">extractCallSigns</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">callSigns</span><span class="o">.</span><span class="n">saveAsTextFile</span><span class="p">(</span><span class="s">"output"</span> <span class="o">+</span> <span class="s">"/callsigns"</span><span class="p">)</span>
<span class="p">[</span><span class="n">Stage</span> <span class="mi">0</span><span class="p">:</span><span class="o">&gt;</span>                                                <span class="p">[</span><span class="n">Stage</span> <span class="mi">0</span><span class="p">:</span><span class="o">&gt;</span>                                                          <span class="p">(</span><span class="mi">0</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">]</span>
<span class="p">[</span><span class="n">Stage</span> <span class="mi">0</span><span class="p">:</span><span class="o">=============================&gt;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span> <span class="s">"Blank Lines: </span><span class="si">%</span><span class="s">d"</span> <span class="o">%</span> <span class="n">blankLines</span><span class="o">.</span><span class="n">value</span>
<span class="n">Blank</span> <span class="n">Lines</span><span class="p">:</span> <span class="mi">2</span>
</code></pre></div></div>
<p>在上面的例子中，我们创建了一个名为blankLines的整型累加器（Accumulator[Int]），初始化为0，然后再每次读到一个空白行的时候blankLines加一。因此，累加器使我们可以用一种更简便的方式，在一个RDD的转换过程中对值进行聚合，而不用额外使用一个filter()或reduce()操作。
需要注意的是，由于Spark的lazy机制，只有在saveAsTestFile这个action算子执行后我们才能得到blankLines的正确结果。</p>

<p><strong>Broadcast Variables 广播变量</strong></p>

<p>Spark的另一种共享变量是广播变量。通常情况下，当一个RDD的很多操作都需要使用driver中定义的变量时，每次操作，driver都要把变量发送给worker节点一次，如果这个变量中的数据很大的话，会产生很高的传输负载，导致执行效率降低。使用广播变量可以使程序高效地将一个很大的只读数据发送给多个worker节点，而且对每个worker节点只需要传输一次，每次操作时executor可以直接获取本地保存的数据副本，不需要多次传输。</p>

<p>Broadcast variables let programmer keep a read-only variable cached on each machine rather than shipping a copy of it with tasks.
For example, to give every node a copy of a large input dataset efficiently.
Spark also attempts to distribute broadcast variables using efficient broadcast algorithms to reduce communication cost.</p>

<h4 id="37-dataframe-and-sql-operations">3.7 DataFrame and SQL Operations</h4>

<p>You can easily use DataFrames and SQL operations on streaming data. You have to create a SparkSession using the SparkContext that the StreamingContext is using. Furthermore this has to done such that it can be restarted on driver failures. This is done by creating a lazily instantiated singleton instance of SparkSession. This is shown in the following example. It modifies the earlier word count example to generate word counts using DataFrames and SQL. Each RDD is converted to a DataFrame, registered as a temporary table and then queried using SQL.</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cm">/** DataFrame operations inside your streaming program */</span>

<span class="k">val</span> <span class="nv">words</span><span class="k">:</span> <span class="kt">DStream</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>

<span class="nv">words</span><span class="o">.</span><span class="py">foreachRDD</span> <span class="o">{</span> <span class="n">rdd</span> <span class="k">=&gt;</span>

  <span class="c1">// Get the singleton instance of SparkSession
</span>  <span class="k">val</span> <span class="nv">spark</span> <span class="k">=</span> <span class="nv">SparkSession</span><span class="o">.</span><span class="py">builder</span><span class="o">.</span><span class="py">config</span><span class="o">(</span><span class="nv">rdd</span><span class="o">.</span><span class="py">sparkContext</span><span class="o">.</span><span class="py">getConf</span><span class="o">).</span><span class="py">getOrCreate</span><span class="o">()</span>
  <span class="k">import</span> <span class="nn">spark.implicits._</span>

  <span class="c1">// Convert RDD[String] to DataFrame
</span>  <span class="k">val</span> <span class="nv">wordsDataFrame</span> <span class="k">=</span> <span class="nv">rdd</span><span class="o">.</span><span class="py">toDF</span><span class="o">(</span><span class="s">"word"</span><span class="o">)</span>

  <span class="c1">// Create a temporary view
</span>  <span class="nv">wordsDataFrame</span><span class="o">.</span><span class="py">createOrReplaceTempView</span><span class="o">(</span><span class="s">"words"</span><span class="o">)</span>

  <span class="c1">// Do word count on DataFrame using SQL and print it
</span>  <span class="k">val</span> <span class="nv">wordCountsDataFrame</span> <span class="k">=</span>
    <span class="nv">spark</span><span class="o">.</span><span class="py">sql</span><span class="o">(</span><span class="s">"select word, count(*) as total from words group by word"</span><span class="o">)</span>
  <span class="nv">wordCountsDataFrame</span><span class="o">.</span><span class="py">show</span><span class="o">()</span>
<span class="o">}</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Lazily instantiated global instance of SparkSession
</span><span class="k">def</span> <span class="nf">getSparkSessionInstance</span><span class="p">(</span><span class="n">sparkConf</span><span class="p">):</span>
    <span class="k">if</span> <span class="p">(</span><span class="s">'sparkSessionSingletonInstance'</span> <span class="ow">not</span> <span class="ow">in</span> <span class="nb">globals</span><span class="p">()):</span>
        <span class="nb">globals</span><span class="p">()[</span><span class="s">'sparkSessionSingletonInstance'</span><span class="p">]</span> <span class="o">=</span> <span class="n">SparkSession</span>\
            <span class="o">.</span><span class="n">builder</span>\
            <span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="n">conf</span><span class="o">=</span><span class="n">sparkConf</span><span class="p">)</span>\
            <span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
    <span class="k">return</span> <span class="nb">globals</span><span class="p">()[</span><span class="s">'sparkSessionSingletonInstance'</span><span class="p">]</span>

<span class="o">...</span>

<span class="c1"># DataFrame operations inside your streaming program
</span>
<span class="n">words</span> <span class="o">=</span> <span class="o">...</span> <span class="c1"># DStream of strings
</span>
<span class="k">def</span> <span class="nf">process</span><span class="p">(</span><span class="n">time</span><span class="p">,</span> <span class="n">rdd</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"========= </span><span class="si">%</span><span class="s">s ========="</span> <span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="n">time</span><span class="p">))</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="c1"># Get the singleton instance of SparkSession
</span>        <span class="n">spark</span> <span class="o">=</span> <span class="n">getSparkSessionInstance</span><span class="p">(</span><span class="n">rdd</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">getConf</span><span class="p">())</span>

        <span class="c1"># Convert RDD[String] to RDD[Row] to DataFrame
</span>        <span class="n">rowRdd</span> <span class="o">=</span> <span class="n">rdd</span><span class="o">.</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">w</span><span class="p">:</span> <span class="n">Row</span><span class="p">(</span><span class="n">word</span><span class="o">=</span><span class="n">w</span><span class="p">))</span>
        <span class="n">wordsDataFrame</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">rowRdd</span><span class="p">)</span>

        <span class="c1"># Creates a temporary view using the DataFrame
</span>        <span class="n">wordsDataFrame</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="p">(</span><span class="s">"words"</span><span class="p">)</span>

        <span class="c1"># Do word count on table using SQL and print it
</span>        <span class="n">wordCountsDataFrame</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s">"select word, count(*) as total from words group by word"</span><span class="p">)</span>
        <span class="n">wordCountsDataFrame</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="k">pass</span>

<span class="n">words</span><span class="o">.</span><span class="n">foreachRDD</span><span class="p">(</span><span class="n">process</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="38-mllib-operations">3.8 MLlib Operations</h4>

<p>You can also easily use machine learning algorithms provided by <a href="http://spark.apache.org/docs/latest/ml-guide.html">MLlib</a>. First of all, there are streaming machine learning algorithms (e.g. <a href="http://spark.apache.org/docs/latest/mllib-linear-methods.html#streaming-linear-regression">Streaming Linear Regression</a>, <a href="http://spark.apache.org/docs/latest/mllib-clustering.html#streaming-k-means">Streaming KMeans</a>, etc.) which can simultaneously learn from the streaming data as well as apply the model on the streaming data. Beyond these, for a much larger class of machine learning algorithms, you can learn a learning model offline (i.e. using historical data) and then apply the model online on streaming data. See the MLlib guide for more details.</p>

<h4 id="39-caching--persistence">3.9 Caching / Persistence</h4>

<p>Similar to RDDs, DStreams also allow developers to persist the stream’s data in memory. That is, using the persist() method on a DStream will automatically persist every RDD of that DStream in memory. This is useful if the data in the DStream will be computed multiple times (e.g., multiple operations on the same data). For window-based operations like reduceByWindow and reduceByKeyAndWindow and state-based operations like updateStateByKey, this is implicitly true. Hence, DStreams generated by window-based operations are automatically persisted in memory, without the developer calling persist().</p>

<p>For input streams that receive data over the network (such as, Kafka, Flume, sockets, etc.), the default persistence level is set to replicate the data to two nodes for fault-tolerance.</p>

<p>Note that, unlike RDDs, the default persistence level of DStreams keeps the data serialized in memory. This is further discussed in the <a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#memory-tuning">Performance Tuning</a> section. More information on different persistence levels can be found in the <a href="http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence">Spark Programming Guide</a>.</p>

<h4 id="310-checkpointing">3.10 Checkpointing</h4>

<p>A streaming application must operate 24/7 and hence must be resilient to failures unrelated to the application logic (e.g., system failures, JVM crashes, etc.). For this to be possible, Spark Streaming needs to <strong>checkpoint</strong> enough information to a fault- tolerant storage system such that it can recover from failures. There are two types of data that are checkpointed.</p>

<p><strong>Metadata checkpointing</strong> - Saving of the information defining the streaming computation to fault-tolerant storage like HDFS. This is used to recover from failure of the node running the driver of the streaming application (discussed in detail later). Metadata includes:</p>

<blockquote>
  <ul>
    <li>Configuration - The configuration that was used to create the streaming application.</li>
    <li>DStream operations - The set of DStream operations that define the streaming application.</li>
    <li>Incomplete batches - Batches whose jobs are queued but have not completed yet.</li>
  </ul>
</blockquote>

<p><strong>Data checkpointing</strong> - Saving of the generated RDDs to reliable storage. This is necessary in some stateful transformations that combine data across multiple batches. In such transformations, the generated RDDs depend on RDDs of previous batches, which causes the length of the dependency chain to keep increasing with time. To avoid such unbounded increases in recovery time (proportional to dependency chain), intermediate RDDs of stateful transformations are periodically checkpointed to reliable storage (e.g. HDFS) to cut off the dependency chains.</p>

<p>To summarize, metadata checkpointing is primarily needed for recovery from driver failures, whereas data or RDD checkpointing is necessary even for basic functioning if stateful transformations are used.</p>

<p><a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#when-to-enable-checkpointing">When to enable Checkpointing</a></p>

<p><a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#how-to-configure-checkpointing">How to configure Checkpointing</a></p>

<h3 id="ref-nc-input--output">Ref: nc input &amp; output</h3>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>hadoop@NN01 ~]<span class="nv">$ </span>nc <span class="nt">-lp</span> 9999
hhh <span class="c"># input</span>
this is a <span class="nb">test</span> <span class="c"># input</span>
hello <span class="c">#output</span>
</code></pre></div></div>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>hadoop@NN01 ~]<span class="nv">$ </span>nc 127.0.0.1 9999
hhh
this is a <span class="nb">test
</span>hello
</code></pre></div></div>

<p>参考资料：</p>

<p><a href="http://www.cnblogs.com/shishanyuan/p/4747735.html">实时流计算Spark Streaming原理介绍</a></p>


                <hr>


                <ul class="pager">
                    
                    <li class="previous">
                        <a href="/blog/2016/09/05/travel-map/" data-toggle="tooltip" data-placement="top" title="travel Map">
                        Previous<br>
                        <span>travel Map</span>
                        </a>
                    </li>
                    
                    
                    <li class="next">
                        <a href="/blog/2016/09/13/Setup-passphraseless-ssh/" data-toggle="tooltip" data-placement="top" title="Setup passphraseless ssh">
                        Next<br>
                        <span>Setup passphraseless ssh</span>
                        </a>
                    </li>
                    
                </ul>
                    <div class="ds-share flat"
                    <div class="ds-thread"
                        data-thread-key="/hadoop/2016/09/08/Streaming_lesson1"
                        data-title="Spark Streaming Lesson 1"
                        data-url="http://localhost:4000/hadoop/2016/09/08/Streaming_lesson1/" >
                    <div class="ds-share-inline">
                      <ul  class="ds-share-icons-16">

                        <li data-toggle="ds-share-icons-more"><a class="ds-more" href="javascript:void(0);">分享到：</a></li>
                        <li><a class="ds-weibo" href="javascript:void(0);" data-service="weibo">微博</a></li>
                        <li><a class="ds-qzone" href="javascript:void(0);" data-service="qzone">QQ空间</a></li>
                        <li><a class="ds-qqt" href="javascript:void(0);" data-service="qqt">腾讯微博</a></li>
                        <li><a class="ds-wechat" href="javascript:void(0);" data-service="wechat">微信</a></li>

                      </ul>
                      <div class="ds-share-icons-more">
                      </div>
                    </div>
                 </div>
                
                <!-- 多说评论框 start -->
                <div class="comment">
                    <div class="ds-thread"
                        data-thread-key="/hadoop/2016/09/08/Streaming_lesson1"
                        data-title="Spark Streaming Lesson 1"
                        data-url="http://localhost:4000/hadoop/2016/09/08/Streaming_lesson1/" >
                    </div>

                </div>
                <!-- 多说评论框 end -->
                

                
                <!-- disqus 评论框 start -->
                <div class="comment">
                    <div id="disqus_thread" class="disqus-thread"></div>
                </div>
                <!-- disqus 评论框 end -->
                

            </div>

    <!-- Side Catalog Container -->
        
            <div class="
                col-lg-2 col-lg-offset-0
                visible-lg-block
                sidebar-container
                catalog-container">
                <div class="side-catalog">
                    <hr class="hidden-sm hidden-xs">
                    <h5>
                        <a class="catalog-toggle" href="#">CATALOG</a>
                    </h5>
                    <ul class="catalog-body"></ul>
                </div>
            </div>
        

    <!-- Sidebar Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                
                <section>
                    <hr class="hidden-sm hidden-xs">
                    <h5><a href="/tags/">FEATURED TAGS</a></h5>
                    <div class="tags">
        				
                            
                				<a href="/tags/#Machine Learning" title="Machine Learning" rel="27">
                                Machine Learning
                                </a>
                            
        				
                            
                				<a href="/tags/#Spark" title="Spark" rel="23">
                                Spark
                                </a>
                            
        				
                            
                				<a href="/tags/#ME" title="ME" rel="4">
                                ME
                                </a>
                            
        				
                            
                				<a href="/tags/#Other" title="Other" rel="4">
                                Other
                                </a>
                            
        				
                            
                				<a href="/tags/#CFAL1" title="CFAL1" rel="9">
                                CFAL1
                                </a>
                            
        				
                            
                				<a href="/tags/#Python" title="Python" rel="26">
                                Python
                                </a>
                            
        				
                            
                				<a href="/tags/#maths" title="maths" rel="2">
                                maths
                                </a>
                            
        				
                            
                				<a href="/tags/#Baby" title="Baby" rel="2">
                                Baby
                                </a>
                            
        				
                            
                				<a href="/tags/#Big Data" title="Big Data" rel="23">
                                Big Data
                                </a>
                            
        				
                            
                				<a href="/tags/#大数据" title="大数据" rel="21">
                                大数据
                                </a>
                            
        				
                            
                				<a href="/tags/#IT" title="IT" rel="10">
                                IT
                                </a>
                            
        				
                            
        				
                            
                				<a href="/tags/#Scala" title="Scala" rel="24">
                                Scala
                                </a>
                            
        				
                            
        				
                            
        				
                            
        				
                            
        				
                            
        				
                            
        				
                            
        				
                            
                				<a href="/tags/#English" title="English" rel="23">
                                English
                                </a>
                            
        				
                            
                				<a href="/tags/#wordcloud" title="wordcloud" rel="3">
                                wordcloud
                                </a>
                            
        				
                            
                				<a href="/tags/#quant" title="quant" rel="3">
                                quant
                                </a>
                            
        				
                            
                				<a href="/tags/#python" title="python" rel="2">
                                python
                                </a>
                            
        				
                            
        				
                            
        				
                            
        				
                            
        				
                            
                				<a href="/tags/#Clustering" title="Clustering" rel="3">
                                Clustering
                                </a>
                            
        				
                            
        				
                            
        				
                            
        				
                            
        				
                            
                				<a href="/tags/#ML" title="ML" rel="3">
                                ML
                                </a>
                            
        				
                            
                				<a href="/tags/#Graphx" title="Graphx" rel="3">
                                Graphx
                                </a>
                            
        				
                            
        				
                            
        				
                            
        				
        			</div>
                </section>
                

                <!-- Friends Blog -->
                
                <hr>
                <h5>FRIENDS</h5>
                <ul class="list-inline">
                    
                        <li><a href="https://www.kaggle.com/">Your Home for Data Science</a></li>
                    
                        <li><a href="https://www.analyticsvidhya.com/">Analytics Vidhya</a></li>
                    
                        <li><a href="https://www.cfainstitute.org/pages/index.aspx">CFA</a></li>
                    
                        <li><a href="http://www.garp.org/#!/home">GARP</a></li>
                    
                        <li><a href="http://www.investopedia.com/">Investopedia</a></li>
                    
                        <li><a href="https://www.quantstart.com/">Quant Start</a></li>
                    
                        <li><a href="http://muchong.com/bbs/">muchong</a></li>
                    
                        <li><a href="https://www.coursera.org/learn/machine-learning">Coursera</a></li>
                    
                </ul>
                
            </div>
        </div>
    </div>
</article>
<script type="text/javascript"  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


<!-- 多说公共JS代码 start (一个网页只需插入一次) -->
<!-- 多说评论框 end -->
<!-- 多说公共JS代码 start (一个网页只需插入一次) -->
<script type="text/javascript">
var duoshuoQuery = {short_name:"machinelearningadvance"};
    (function() {
        var ds = document.createElement('script');
        ds.type = 'text/javascript';ds.async = true;
        ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
        ds.charset = 'UTF-8';
        (document.getElementsByTagName('head')[0]
         || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
    </script>
<!-- 多说公共JS代码 end -->
<!-- 多说公共JS代码 end -->




<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("http://cdn.bootcss.com/anchor-js/1.1.1/anchor.min.js",function(){
        anchors.options = {
          visible: 'always',
          placement: 'right',
          icon: '#'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>
<style>
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>



    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                    
                    <li>
                        <a href="/feed.xml">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-rss fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    
                    

                    <!-- add Weibo, Zhihu by Hux, add target = "_blank" to <a> by Hux -->
                    
                    
                    <li>
                        <a target="_blank" href="http://weibo.com/u/2672280861">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-weibo fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    


                    
                    <li>
                        <a target="_blank" href="https://www.facebook.com/davidyjun">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-facebook fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li>
                        <a target="_blank" href="https://github.com/helloourworld">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li>
                        <a target="_blank" href="https://www.linkedin.com/in/lijun-yu-13b9b475">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    
                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; Big Data Memo 2021
                </p>
            </div>
        </div>
    </div>
</footer>

<!-- jQuery -->
<script src="/js/jquery.min.js "></script>

<!-- Bootstrap Core JavaScript -->
<script src="/js/bootstrap.min.js "></script>

<!-- Custom Theme JavaScript -->
<script src="/js/hux-blog.min.js "></script>


<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>

<!--
     Because of the native support for backtick-style fenced code blocks
     right within the Markdown is landed in Github Pages,
     From V1.6, There is no need for Highlight.js,
     so Huxblog drops it officially.

     - https://github.com/blog/2100-github-pages-now-faster-and-simpler-with-jekyll-3-0
     - https://help.github.com/articles/creating-and-highlighting-code-blocks/
     - https://github.com/jneen/rouge/wiki/list-of-supported-languages-and-lexers
-->
<!--
    <script>
        async("http://cdn.bootcss.com/highlight.js/8.6/highlight.min.js", function(){
            hljs.initHighlightingOnLoad();
        })
    </script>
    <link href="http://cdn.bootcss.com/highlight.js/8.6/styles/github.min.css" rel="stylesheet">
-->


<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async('/js/jquery.tagcloud.js',function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("http://cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->

<script>
    // dynamic User by Hux
    var _gaId = 'UA-82819752-1';
    var _gaDomain = 'machinelearningadvance.com';

    // Originial
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', _gaId, _gaDomain);
    ga('send', 'pageview');
</script>



<!-- Baidu Tongji -->

<script>
    // dynamic User by Hux
    var _baId = '863b1f46c83a8e14e47782106aee7e7f';

    // Originial
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?" + _baId;
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
</script>




<!-- Side Catalog -->

<script type="text/javascript">
    function generateCatalog (selector) {
        var P = $('div.post-container'),a,n,t,l,i,c;
        a = P.find('h1,h2,h3,h4,h5,h6');
        a.each(function () {
            n = $(this).prop('tagName').toLowerCase();
            i = "#"+$(this).prop('id');
            t = $(this).text();
            c = $('<a href="'+i+'" rel="nofollow">'+t+'</a>');
            l = $('<li class="'+n+'_nav"></li>').append(c);
            $(selector).append(l);
        });
        return true;
    }

    generateCatalog(".catalog-body");

    // toggle side catalog
    $(".catalog-toggle").click((function(e){
        e.preventDefault();
        $('.side-catalog').toggleClass("fold")
    }))

    /*
     * Doc: https://github.com/davist11/jQuery-One-Page-Nav
     * Fork by Hux to support padding
     */
    async("/js/jquery.nav.js", function () {
        $('.catalog-body').onePageNav({
            currentClass: "active",
            changeHash: !1,
            easing: "swing",
            filter: "",
            scrollSpeed: 700,
            scrollOffset: 0,
            scrollThreshold: .2,
            begin: null,
            end: null,
            scrollChange: null,
            padding: 80
        });
    });
</script>





<!-- Image to hack wechat -->
<img src="/images/background.jpg" width="0" height="0" />
<!-- Migrate from head to bottom, no longer block render and still work -->

</body>

</html>
