<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="baidu-site-verification" content="g89MuzujW3" />
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="关于机器学习，关于大数据，关于你身边的金融，关于我们的生活... | LijunYu">
    <meta name="keywords"  content="LijunYu, 包包的老公, 大数据, 机器学习, CFA, Machine Learning(Deep Learning), Big Data, Spark">
    <link rel="stylesheet" href="/css/default.css" type="text/css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
    <link rel="alternate" type="application/atom+xml" title="Recent Entries" href="/atom.xml" />
    <link rel="alternatecss" type="application/rss+xml" title="“Lijun Yu's Blog”" href="http://helloourworld.github.io/feed.xml">
    <script src="/js/jquery-1.7.1.min.js" type="text/javascript"></script>

    <title>Spark-ML-0304-Ensembles-RF - Maching Learning | 机器学习笔记</title>

    <link rel="canonical" href="http://machinelearningadvance.com/mladvance/2017/04/06/random-forests/">

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/hux-blog.min.css">

    <!-- Pygments Github CSS -->
    <link rel="stylesheet" href="/css/syntax.css">

    <!-- Custom Fonts -->
    <link href="http://cdn.staticfile.org/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">

    <!-- ga & ba script hoook -->
    <script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>

</head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">

    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Big Data Memo</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>
                    
                    <li>
                        <a href="/about/">About</a>
                    </li>
                    
                    <li>
                        <a href="/category/">Category</a>
                    </li>
                    
                    <li>
                        <a href="/tags/">Tags</a>
                    </li>
                    
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    var __HuxNav__ = {
        close: function(){
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        },
        open: function(){
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }

    // Bind Event
    $toggle.addEventListener('click', function(e){
        if ($navbar.className.indexOf('in') > 0) {
            __HuxNav__.close()
        }else{
            __HuxNav__.open()
        }
    })

    /**
     * Since Fastclick is used to delegate 'touchstart' globally
     * to hack 300ms delay in iOS by performing a fake 'click',
     * Using 'e.stopPropagation' to stop 'touchstart' event from 
     * $toggle/$collapse will break global delegation.
     * 
     * Instead, we use a 'e.target' filter to prevent handler
     * added to document close HuxNav.  
     *
     * Also, we use 'click' instead of 'touchstart' as compromise
     */
    document.addEventListener('click', function(e){
        if(e.target == $toggle) return;
        if(e.target.className == 'icon-bar') return;
        __HuxNav__.close();
    })
</script>


    <!-- Image to hack wechat -->
<!-- <img src="/images/background.jpg" width="0" height="0"> -->

<!-- Post Header -->
<style type="text/css">
    header.intro-header{
        position: relative;
        background-image: url('/images/background.jpg')
    }

    
</style>
<header class="intro-header" >
    <div class="header-mask"></div>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <div class="tags">
                        
                        <a class="tag" href="/tags/#Machine Learning" title="Machine Learning">Machine Learning</a>
                        
                        <a class="tag" href="/tags/#Python" title="Python">Python</a>
                        
                    </div>
                    <h1>Spark-ML-0304-Ensembles-RF</h1>
                    
                    
                    <h2 class="subheading"></h2>
                    
                    <span class="meta">Posted by Big Data Memo on April 6, 2017</span>
                </div>
            </div>
        </div>
    </div>
</header>

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

    <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

				<h1 id="section">随机森林</h1>

<h2 id="bagging">1 Bagging</h2>

<p>  <code class="highlighter-rouge">Bagging</code>采用自助采样法(<code class="highlighter-rouge">bootstrap sampling</code>)采样数据。给定包含<code class="highlighter-rouge">m</code>个样本的数据集，我们先随机取出一个样本放入采样集中，再把该样本放回初始数据集，使得下次采样时，样本仍可能被选中，
这样，经过<code class="highlighter-rouge">m</code>次随机采样操作，我们得到包含<code class="highlighter-rouge">m</code>个样本的采样集。</p>

<p>  按照此方式，我们可以采样出<code class="highlighter-rouge">T</code>个含<code class="highlighter-rouge">m</code>个训练样本的采样集，然后基于每个采样集训练出一个基本学习器，再将这些基本学习器进行结合。这就是<code class="highlighter-rouge">Bagging</code>的一般流程。在对预测输出进行结合时，<code class="highlighter-rouge">Bagging</code>通常使用简单投票法，
对回归问题使用简单平均法。若分类预测时，出现两个类收到同样票数的情形，则最简单的做法是随机选择一个，也可以进一步考察学习器投票的置信度来确定最终胜者。</p>

<p>  <code class="highlighter-rouge">Bagging</code>的算法描述如下图所示。</p>

<div align="center"><img src="/images/spark/ml/RF/1.1.png" width="400" height="220" alt="1.1" align="center" /></div>

<h2 id="section-1">2 随机森林</h2>

<p>  随机森林是<code class="highlighter-rouge">Bagging</code>的一个扩展变体。随机森林在以决策树为基学习器构建<code class="highlighter-rouge">Bagging</code>集成的基础上，进一步在决策树的训练过程中引入了随机属性选择。具体来讲，传统决策树在选择划分属性时，
在当前节点的属性集合（假设有<code class="highlighter-rouge">d</code>个属性）中选择一个最优属性；而在随机森林中，对基决策树的每个节点，先从该节点的属性集合中随机选择一个包含<code class="highlighter-rouge">k</code>个属性的子集，然后再从这个子集中选择一个最优属性用于划分。
这里的参数<code class="highlighter-rouge">k</code>控制了随机性的引入程度。若令<code class="highlighter-rouge">k=d</code>，则基决策树的构建与传统决策树相同；若令<code class="highlighter-rouge">k=1</code>，则是随机选择一个属性用于划分。在<code class="highlighter-rouge">MLlib</code>中，有两种选择用于分类，即<code class="highlighter-rouge">k=log2(d)</code>、<code class="highlighter-rouge">k=sqrt(d)</code>；
一种选择用于回归，即<code class="highlighter-rouge">k=1/3d</code>。在源码分析中会详细介绍。</p>

<p>  可以看出，随机森林对<code class="highlighter-rouge">Bagging</code>只做了小改动，但是与<code class="highlighter-rouge">Bagging</code>中基学习器的“多样性”仅仅通过样本扰动（通过对初始训练集采样）而来不同，随机森林中基学习器的多样性不仅来自样本扰动，还来自属性扰动。
这使得最终集成的泛化性能可通过个体学习器之间差异度的增加而进一步提升。</p>

<h2 id="section-2">3 随机森林在分布式环境下的优化策略</h2>

<p>  随机森林算法在单机环境下很容易实现，但在分布式环境下特别是在<code class="highlighter-rouge">Spark</code>平台上，传统单机形式的迭代方式必须要进行相应改进才能适用于分布式环境
，这是因为在分布式环境下，数据也是分布式的，算法设计不得当会生成大量的<code class="highlighter-rouge">IO</code>操作，例如频繁的网络数据传输，从而影响算法效率。
因此，在<code class="highlighter-rouge">Spark</code>上进行随机森林算法的实现，需要进行一定的优化，<code class="highlighter-rouge">Spark</code>中的随机森林算法主要实现了三个优化策略：</p>

<ul>
  <li>切分点抽样统计，如下图所示。在单机环境下的决策树对连续变量进行切分点选择时，一般是通过对特征点进行排序，然后取相邻两个数之间的点作为切分点，这在单机环境下是可行的，但如果在分布式环境下如此操作的话，
会带来大量的网络传输操作，特别是当数据量达到<code class="highlighter-rouge">PB</code>级时，算法效率将极为低下。为避免该问题，<code class="highlighter-rouge">Spark</code>中的随机森林在构建决策树时，会对各分区采用一定的子特征策略进行抽样，然后生成各个分区的统计数据，并最终得到切分点。
(从源代码里面看，是先对样本进行抽样，然后根据抽样样本值出现的次数进行排序，然后再进行切分)。</li>
</ul>

<div align="center"><img src="/images/spark/ml/RF/1.2.png" width="600" height="350" alt="1.2" align="center" /></div>

<ul>
  <li>特征装箱（<code class="highlighter-rouge">Binning</code>），如下图所示。决策树的构建过程就是对特征的取值不断进行划分的过程，对于离散的特征，如果有<code class="highlighter-rouge">M</code>个值，最多有<code class="highlighter-rouge">2^(M-1) - 1</code>个划分。如果值是有序的，那么就最多<code class="highlighter-rouge">M-1</code>个划分。
比如年龄特征，有老，中，少3个值，如果无序有<code class="highlighter-rouge">2^2-1=3</code>个划分，即<code class="highlighter-rouge">老|中，少；老，中|少；老，少|中</code>。；如果是有序的，即按老，中，少的序，那么只有<code class="highlighter-rouge">m-1</code>个，即2种划分，<code class="highlighter-rouge">老|中，少；老，中|少</code>。
对于连续的特征，其实就是进行范围划分，而划分的点就是<code class="highlighter-rouge">split</code>（切分点），划分出的区间就是<code class="highlighter-rouge">bin</code>。对于连续特征，理论上<code class="highlighter-rouge">split</code>是无数的，在分布环境下不可能取出所有的值，因此它采用的是切点抽样统计方法。</li>
</ul>

<div align="center"><img src="/images/spark/ml/RF/1.3.png" width="600" height="400" alt="1.3" align="center" /></div>

<ul>
  <li>逐层训练（<code class="highlighter-rouge">level-wise training</code>），如下图所示。单机版本的决策树生成过程是通过递归调用（本质上是深度优先）的方式构造树，在构造树的同时，需要移动数据，将同一个子节点的数据移动到一起。
此方法在分布式数据结构上无法有效的执行，而且也无法执行，因为数据太大，无法放在一起，所以在分布式环境下采用的策略是逐层构建树节点（本质上是广度优先），这样遍历所有数据的次数等于所有树中的最大层数。
每次遍历时，只需要计算每个节点所有切分点统计参数，遍历完后，根据节点的特征划分，决定是否切分，以及如何切分。</li>
</ul>

<div align="center"><img src="/images/spark/ml/RF/1.4.png" width="600" height="350" alt="1.4" align="center" /></div>

<h2 id="section-3">4 使用实例</h2>

<p>  下面的例子用于分类。</p>

<div class="language-scala highlighter-rouge"><pre class="highlight"><code><span class="k">import</span> <span class="nn">org.apache.spark.mllib.tree.RandomForest</span>
<span class="k">import</span> <span class="nn">org.apache.spark.mllib.tree.model.RandomForestModel</span>
<span class="k">import</span> <span class="nn">org.apache.spark.mllib.util.MLUtils</span>
<span class="c1">// Load and parse the data file.
</span><span class="k">val</span> <span class="n">data</span> <span class="k">=</span> <span class="nc">MLUtils</span><span class="o">.</span><span class="n">loadLibSVMFile</span><span class="o">(</span><span class="n">sc</span><span class="o">,</span> <span class="s">"data/mllib/sample_libsvm_data.txt"</span><span class="o">)</span>
<span class="c1">// Split the data into training and test sets (30% held out for testing)
</span><span class="k">val</span> <span class="n">splits</span> <span class="k">=</span> <span class="n">data</span><span class="o">.</span><span class="n">randomSplit</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span><span class="mf">0.7</span><span class="o">,</span> <span class="mf">0.3</span><span class="o">))</span>
<span class="k">val</span> <span class="o">(</span><span class="n">trainingData</span><span class="o">,</span> <span class="n">testData</span><span class="o">)</span> <span class="k">=</span> <span class="o">(</span><span class="n">splits</span><span class="o">(</span><span class="mi">0</span><span class="o">),</span> <span class="n">splits</span><span class="o">(</span><span class="mi">1</span><span class="o">))</span>
<span class="c1">// Train a RandomForest model.
// 空的类别特征信息表示所有的特征都是连续的.
</span><span class="k">val</span> <span class="n">numClasses</span> <span class="k">=</span> <span class="mi">2</span>
<span class="k">val</span> <span class="n">categoricalFeaturesInfo</span> <span class="k">=</span> <span class="nc">Map</span><span class="o">[</span><span class="kt">Int</span>, <span class="kt">Int</span><span class="o">]()</span>
<span class="k">val</span> <span class="n">numTrees</span> <span class="k">=</span> <span class="mi">3</span> <span class="c1">// Use more in practice.
</span><span class="k">val</span> <span class="n">featureSubsetStrategy</span> <span class="k">=</span> <span class="s">"auto"</span> <span class="c1">// Let the algorithm choose.
</span><span class="k">val</span> <span class="n">impurity</span> <span class="k">=</span> <span class="s">"gini"</span>
<span class="k">val</span> <span class="n">maxDepth</span> <span class="k">=</span> <span class="mi">4</span>
<span class="k">val</span> <span class="n">maxBins</span> <span class="k">=</span> <span class="mi">32</span>
<span class="k">val</span> <span class="n">model</span> <span class="k">=</span> <span class="nc">RandomForest</span><span class="o">.</span><span class="n">trainClassifier</span><span class="o">(</span><span class="n">trainingData</span><span class="o">,</span> <span class="n">numClasses</span><span class="o">,</span> <span class="n">categoricalFeaturesInfo</span><span class="o">,</span>
  <span class="n">numTrees</span><span class="o">,</span> <span class="n">featureSubsetStrategy</span><span class="o">,</span> <span class="n">impurity</span><span class="o">,</span> <span class="n">maxDepth</span><span class="o">,</span> <span class="n">maxBins</span><span class="o">)</span>
<span class="c1">// Evaluate model on test instances and compute test error
</span><span class="k">val</span> <span class="n">labelAndPreds</span> <span class="k">=</span> <span class="n">testData</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="n">point</span> <span class="k">=&gt;</span>
  <span class="k">val</span> <span class="n">prediction</span> <span class="k">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="o">(</span><span class="n">point</span><span class="o">.</span><span class="n">features</span><span class="o">)</span>
  <span class="o">(</span><span class="n">point</span><span class="o">.</span><span class="n">label</span><span class="o">,</span> <span class="n">prediction</span><span class="o">)</span>
<span class="o">}</span>
<span class="k">val</span> <span class="n">testErr</span> <span class="k">=</span> <span class="n">labelAndPreds</span><span class="o">.</span><span class="n">filter</span><span class="o">(</span><span class="n">r</span> <span class="k">=&gt;</span> <span class="n">r</span><span class="o">.</span><span class="n">_1</span> <span class="o">!=</span> <span class="n">r</span><span class="o">.</span><span class="n">_2</span><span class="o">).</span><span class="n">count</span><span class="o">.</span><span class="n">toDouble</span> <span class="o">/</span> <span class="n">testData</span><span class="o">.</span><span class="n">count</span><span class="o">()</span>
<span class="n">println</span><span class="o">(</span><span class="s">"Test Error = "</span> <span class="o">+</span> <span class="n">testErr</span><span class="o">)</span>
<span class="n">println</span><span class="o">(</span><span class="s">"Learned classification forest model:\n"</span> <span class="o">+</span> <span class="n">model</span><span class="o">.</span><span class="n">toDebugString</span><span class="o">)</span>
</code></pre>
</div>

<p>  下面的例子用于回归。</p>

<div class="language-scala highlighter-rouge"><pre class="highlight"><code><span class="k">import</span> <span class="nn">org.apache.spark.mllib.tree.RandomForest</span>
<span class="k">import</span> <span class="nn">org.apache.spark.mllib.tree.model.RandomForestModel</span>
<span class="k">import</span> <span class="nn">org.apache.spark.mllib.util.MLUtils</span>
<span class="c1">// Load and parse the data file.
</span><span class="k">val</span> <span class="n">data</span> <span class="k">=</span> <span class="nc">MLUtils</span><span class="o">.</span><span class="n">loadLibSVMFile</span><span class="o">(</span><span class="n">sc</span><span class="o">,</span> <span class="s">"data/mllib/sample_libsvm_data.txt"</span><span class="o">)</span>
<span class="c1">// Split the data into training and test sets (30% held out for testing)
</span><span class="k">val</span> <span class="n">splits</span> <span class="k">=</span> <span class="n">data</span><span class="o">.</span><span class="n">randomSplit</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span><span class="mf">0.7</span><span class="o">,</span> <span class="mf">0.3</span><span class="o">))</span>
<span class="k">val</span> <span class="o">(</span><span class="n">trainingData</span><span class="o">,</span> <span class="n">testData</span><span class="o">)</span> <span class="k">=</span> <span class="o">(</span><span class="n">splits</span><span class="o">(</span><span class="mi">0</span><span class="o">),</span> <span class="n">splits</span><span class="o">(</span><span class="mi">1</span><span class="o">))</span>
<span class="c1">// Train a RandomForest model.
// 空的类别特征信息表示所有的特征都是连续的
</span><span class="k">val</span> <span class="n">numClasses</span> <span class="k">=</span> <span class="mi">2</span>
<span class="k">val</span> <span class="n">categoricalFeaturesInfo</span> <span class="k">=</span> <span class="nc">Map</span><span class="o">[</span><span class="kt">Int</span>, <span class="kt">Int</span><span class="o">]()</span>
<span class="k">val</span> <span class="n">numTrees</span> <span class="k">=</span> <span class="mi">3</span> <span class="c1">// Use more in practice.
</span><span class="k">val</span> <span class="n">featureSubsetStrategy</span> <span class="k">=</span> <span class="s">"auto"</span> <span class="c1">// Let the algorithm choose.
</span><span class="k">val</span> <span class="n">impurity</span> <span class="k">=</span> <span class="s">"variance"</span>
<span class="k">val</span> <span class="n">maxDepth</span> <span class="k">=</span> <span class="mi">4</span>
<span class="k">val</span> <span class="n">maxBins</span> <span class="k">=</span> <span class="mi">32</span>
<span class="k">val</span> <span class="n">model</span> <span class="k">=</span> <span class="nc">RandomForest</span><span class="o">.</span><span class="n">trainRegressor</span><span class="o">(</span><span class="n">trainingData</span><span class="o">,</span> <span class="n">categoricalFeaturesInfo</span><span class="o">,</span>
  <span class="n">numTrees</span><span class="o">,</span> <span class="n">featureSubsetStrategy</span><span class="o">,</span> <span class="n">impurity</span><span class="o">,</span> <span class="n">maxDepth</span><span class="o">,</span> <span class="n">maxBins</span><span class="o">)</span>
<span class="c1">// Evaluate model on test instances and compute test error
</span><span class="k">val</span> <span class="n">labelsAndPredictions</span> <span class="k">=</span> <span class="n">testData</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="n">point</span> <span class="k">=&gt;</span>
  <span class="k">val</span> <span class="n">prediction</span> <span class="k">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="o">(</span><span class="n">point</span><span class="o">.</span><span class="n">features</span><span class="o">)</span>
  <span class="o">(</span><span class="n">point</span><span class="o">.</span><span class="n">label</span><span class="o">,</span> <span class="n">prediction</span><span class="o">)</span>
<span class="o">}</span>
<span class="k">val</span> <span class="n">testMSE</span> <span class="k">=</span> <span class="n">labelsAndPredictions</span><span class="o">.</span><span class="n">map</span><span class="o">{</span> <span class="k">case</span><span class="o">(</span><span class="n">v</span><span class="o">,</span> <span class="n">p</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">math</span><span class="o">.</span><span class="n">pow</span><span class="o">((</span><span class="n">v</span> <span class="o">-</span> <span class="n">p</span><span class="o">),</span> <span class="mi">2</span><span class="o">)}.</span><span class="n">mean</span><span class="o">()</span>
<span class="n">println</span><span class="o">(</span><span class="s">"Test Mean Squared Error = "</span> <span class="o">+</span> <span class="n">testMSE</span><span class="o">)</span>
<span class="n">println</span><span class="o">(</span><span class="s">"Learned regression forest model:\n"</span> <span class="o">+</span> <span class="n">model</span><span class="o">.</span><span class="n">toDebugString</span><span class="o">)</span>
</code></pre>
</div>

<h2 id="section-4">5 源码分析</h2>

<h3 id="section-5">5.1 训练分析</h3>

<p>  训练过程简单可以分为两步，第一步是初始化，第二步是迭代构建随机森林。这两大步还分为若干小步，下面会分别介绍这些内容。</p>

<h4 id="section-6">5.1.1 初始化</h4>

<div class="language-scala highlighter-rouge"><pre class="highlight"><code><span class="k">val</span> <span class="n">retaggedInput</span> <span class="k">=</span> <span class="n">input</span><span class="o">.</span><span class="n">retag</span><span class="o">(</span><span class="n">classOf</span><span class="o">[</span><span class="kt">LabeledPoint</span><span class="o">])</span>
<span class="c1">//建立决策树的元数据信息（分裂点位置、箱子数及各箱子包含特征属性的值等）
</span><span class="k">val</span> <span class="n">metadata</span> <span class="k">=</span>
    <span class="nc">DecisionTreeMetadata</span><span class="o">.</span><span class="n">buildMetadata</span><span class="o">(</span><span class="n">retaggedInput</span><span class="o">,</span> <span class="n">strategy</span><span class="o">,</span> <span class="n">numTrees</span><span class="o">,</span> <span class="n">featureSubsetStrategy</span><span class="o">)</span>
<span class="c1">//找到切分点（splits）及箱子信息（Bins）
//对于连续型特征，利用切分点抽样统计简化计算
//对于离散型特征，如果是无序的，则最多有个 splits=2^(numBins-1)-1 划分
//如果是有序的，则最多有 splits=numBins-1 个划分
</span><span class="k">val</span> <span class="o">(</span><span class="n">splits</span><span class="o">,</span> <span class="n">bins</span><span class="o">)</span> <span class="k">=</span> <span class="nc">DecisionTree</span><span class="o">.</span><span class="n">findSplitsBins</span><span class="o">(</span><span class="n">retaggedInput</span><span class="o">,</span> <span class="n">metadata</span><span class="o">)</span>
<span class="c1">//转换成树形的 RDD 类型，转换后，所有样本点已经按分裂点条件分到了各自的箱子中
</span><span class="k">val</span> <span class="n">treeInput</span> <span class="k">=</span> <span class="nc">TreePoint</span><span class="o">.</span><span class="n">convertToTreeRDD</span><span class="o">(</span><span class="n">retaggedInput</span><span class="o">,</span> <span class="n">bins</span><span class="o">,</span> <span class="n">metadata</span><span class="o">)</span>
<span class="k">val</span> <span class="n">withReplacement</span> <span class="k">=</span> <span class="k">if</span> <span class="o">(</span><span class="n">numTrees</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="o">)</span> <span class="kc">true</span> <span class="k">else</span> <span class="kc">false</span>
<span class="c1">// convertToBaggedRDD 方法使得每棵树就是样本的一个子集
</span><span class="k">val</span> <span class="n">baggedInput</span> <span class="k">=</span> <span class="nc">BaggedPoint</span><span class="o">.</span><span class="n">convertToBaggedRDD</span><span class="o">(</span><span class="n">treeInput</span><span class="o">,</span>
          <span class="n">strategy</span><span class="o">.</span><span class="n">subsamplingRate</span><span class="o">,</span> <span class="n">numTrees</span><span class="o">,</span>
          <span class="n">withReplacement</span><span class="o">,</span> <span class="n">seed</span><span class="o">).</span><span class="n">persist</span><span class="o">(</span><span class="nc">StorageLevel</span><span class="o">.</span><span class="nc">MEMORY_AND_DISK</span><span class="o">)</span>
<span class="c1">//决策树的深度，最大为30
</span><span class="k">val</span> <span class="n">maxDepth</span> <span class="k">=</span> <span class="n">strategy</span><span class="o">.</span><span class="n">maxDepth</span>
<span class="c1">//聚合的最大内存
</span><span class="k">val</span> <span class="n">maxMemoryUsage</span><span class="k">:</span> <span class="kt">Long</span> <span class="o">=</span> <span class="n">strategy</span><span class="o">.</span><span class="n">maxMemoryInMB</span> <span class="o">*</span> <span class="mi">1024L</span> <span class="o">*</span> <span class="mi">1024L</span>
<span class="k">val</span> <span class="n">maxMemoryPerNode</span> <span class="k">=</span> <span class="o">{</span>
    <span class="k">val</span> <span class="n">featureSubset</span><span class="k">:</span> <span class="kt">Option</span><span class="o">[</span><span class="kt">Array</span><span class="o">[</span><span class="kt">Int</span><span class="o">]]</span> <span class="k">=</span> <span class="k">if</span> <span class="o">(</span><span class="n">metadata</span><span class="o">.</span><span class="n">subsamplingFeatures</span><span class="o">)</span> <span class="o">{</span>
        <span class="c1">// Find numFeaturesPerNode largest bins to get an upper bound on memory usage.
</span>        <span class="nc">Some</span><span class="o">(</span><span class="n">metadata</span><span class="o">.</span><span class="n">numBins</span><span class="o">.</span><span class="n">zipWithIndex</span><span class="o">.</span><span class="n">sortBy</span><span class="o">(-</span> <span class="k">_</span><span class="o">.</span><span class="n">_1</span><span class="o">)</span>
          <span class="o">.</span><span class="n">take</span><span class="o">(</span><span class="n">metadata</span><span class="o">.</span><span class="n">numFeaturesPerNode</span><span class="o">).</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">_2</span><span class="o">))</span>
    <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
        <span class="nc">None</span>
    <span class="o">}</span>
    <span class="c1">//计算聚合操作时节点的内存
</span>    <span class="nc">RandomForest</span><span class="o">.</span><span class="n">aggregateSizeForNode</span><span class="o">(</span><span class="n">metadata</span><span class="o">,</span> <span class="n">featureSubset</span><span class="o">)</span> <span class="o">*</span> <span class="mi">8L</span>
<span class="o">}</span>
</code></pre>
</div>

<p>  初始化的第一步就是决策树元数据信息的构建。它的代码如下所示。</p>

<div class="language-scala highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="n">buildMetadata</span><span class="o">(</span>
      <span class="n">input</span><span class="k">:</span> <span class="kt">RDD</span><span class="o">[</span><span class="kt">LabeledPoint</span><span class="o">],</span>
      <span class="n">strategy</span><span class="k">:</span> <span class="kt">Strategy</span><span class="o">,</span>
      <span class="n">numTrees</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span>
      <span class="n">featureSubsetStrategy</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span><span class="k">:</span> <span class="kt">DecisionTreeMetadata</span> <span class="o">=</span> <span class="o">{</span>
    <span class="c1">//特征数
</span>    <span class="k">val</span> <span class="n">numFeatures</span> <span class="k">=</span> <span class="n">input</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">features</span><span class="o">.</span><span class="n">size</span><span class="o">).</span><span class="n">take</span><span class="o">(</span><span class="mi">1</span><span class="o">).</span><span class="n">headOption</span><span class="o">.</span><span class="n">getOrElse</span> <span class="o">{</span>
      <span class="k">throw</span> <span class="k">new</span> <span class="nc">IllegalArgumentException</span><span class="o">(</span><span class="n">s</span><span class="s">"DecisionTree requires size of input RDD &gt; 0, "</span> <span class="o">+</span>
        <span class="n">s</span><span class="s">"but was given by empty one."</span><span class="o">)</span>
    <span class="o">}</span>
    <span class="k">val</span> <span class="n">numExamples</span> <span class="k">=</span> <span class="n">input</span><span class="o">.</span><span class="n">count</span><span class="o">()</span>
    <span class="k">val</span> <span class="n">numClasses</span> <span class="k">=</span> <span class="n">strategy</span><span class="o">.</span><span class="n">algo</span> <span class="k">match</span> <span class="o">{</span>
      <span class="k">case</span> <span class="nc">Classification</span> <span class="k">=&gt;</span> <span class="n">strategy</span><span class="o">.</span><span class="n">numClasses</span>
      <span class="k">case</span> <span class="nc">Regression</span> <span class="k">=&gt;</span> <span class="mi">0</span>
    <span class="o">}</span>
    <span class="c1">//最大可能的装箱数
</span>    <span class="k">val</span> <span class="n">maxPossibleBins</span> <span class="k">=</span> <span class="n">math</span><span class="o">.</span><span class="n">min</span><span class="o">(</span><span class="n">strategy</span><span class="o">.</span><span class="n">maxBins</span><span class="o">,</span> <span class="n">numExamples</span><span class="o">).</span><span class="n">toInt</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">maxPossibleBins</span> <span class="o">&lt;</span> <span class="n">strategy</span><span class="o">.</span><span class="n">maxBins</span><span class="o">)</span> <span class="o">{</span>
      <span class="n">logWarning</span><span class="o">(</span><span class="n">s</span><span class="s">"DecisionTree reducing maxBins from ${strategy.maxBins} to $maxPossibleBins"</span> <span class="o">+</span>
        <span class="n">s</span><span class="s">" (= number of training instances)"</span><span class="o">)</span>
    <span class="o">}</span>
    <span class="c1">// We check the number of bins here against maxPossibleBins.
</span>    <span class="c1">// This needs to be checked here instead of in Strategy since maxPossibleBins can be modified
</span>    <span class="c1">// based on the number of training examples.
</span>    <span class="c1">//最大分类数要小于最大可能装箱数
</span>    <span class="c1">//这里categoricalFeaturesInfo是传入的信息，这个map保存特征的类别信息。
</span>    <span class="c1">//例如，(n-&gt;k)表示特征k包含的类别有（0,1,...,k-1）
</span>    <span class="k">if</span> <span class="o">(</span><span class="n">strategy</span><span class="o">.</span><span class="n">categoricalFeaturesInfo</span><span class="o">.</span><span class="n">nonEmpty</span><span class="o">)</span> <span class="o">{</span>
      <span class="k">val</span> <span class="n">maxCategoriesPerFeature</span> <span class="k">=</span> <span class="n">strategy</span><span class="o">.</span><span class="n">categoricalFeaturesInfo</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">max</span>
      <span class="k">val</span> <span class="n">maxCategory</span> <span class="k">=</span>
        <span class="n">strategy</span><span class="o">.</span><span class="n">categoricalFeaturesInfo</span><span class="o">.</span><span class="n">find</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">_2</span> <span class="o">==</span> <span class="n">maxCategoriesPerFeature</span><span class="o">).</span><span class="n">get</span><span class="o">.</span><span class="n">_1</span>
      <span class="n">require</span><span class="o">(</span><span class="n">maxCategoriesPerFeature</span> <span class="o">&lt;=</span> <span class="n">maxPossibleBins</span><span class="o">,</span>
        <span class="n">s</span><span class="s">"DecisionTree requires maxBins (= $maxPossibleBins) to be at least as large as the "</span> <span class="o">+</span>
        <span class="n">s</span><span class="s">"number of values in each categorical feature, but categorical feature $maxCategory "</span> <span class="o">+</span>
        <span class="n">s</span><span class="s">"has $maxCategoriesPerFeature values. Considering remove this and other categorical "</span> <span class="o">+</span>
        <span class="s">"features with a large number of values, or add more training examples."</span><span class="o">)</span>
    <span class="o">}</span>
    <span class="k">val</span> <span class="n">unorderedFeatures</span> <span class="k">=</span> <span class="k">new</span> <span class="n">mutable</span><span class="o">.</span><span class="nc">HashSet</span><span class="o">[</span><span class="kt">Int</span><span class="o">]()</span>
    <span class="k">val</span> <span class="n">numBins</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">.</span><span class="n">fill</span><span class="o">[</span><span class="kt">Int</span><span class="o">](</span><span class="n">numFeatures</span><span class="o">)(</span><span class="n">maxPossibleBins</span><span class="o">)</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">numClasses</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="o">)</span> <span class="o">{</span>
      <span class="c1">// 多分类
</span>      <span class="k">val</span> <span class="n">maxCategoriesForUnorderedFeature</span> <span class="k">=</span>
        <span class="o">((</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="o">(</span><span class="n">maxPossibleBins</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span><span class="o">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="o">(</span><span class="mf">2.0</span><span class="o">))</span> <span class="o">+</span> <span class="mi">1</span><span class="o">).</span><span class="n">floor</span><span class="o">.</span><span class="n">toInt</span>
      <span class="n">strategy</span><span class="o">.</span><span class="n">categoricalFeaturesInfo</span><span class="o">.</span><span class="n">foreach</span> <span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="n">featureIndex</span><span class="o">,</span> <span class="n">numCategories</span><span class="o">)</span> <span class="k">=&gt;</span>
        <span class="c1">//如果类别特征只有1个类，我们把它看成连续的特征
</span>        <span class="k">if</span> <span class="o">(</span><span class="n">numCategories</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="o">)</span> <span class="o">{</span>
          <span class="c1">// Decide if some categorical features should be treated as unordered features,
</span>          <span class="c1">//  which require 2 * ((1 &lt;&lt; numCategories - 1) - 1) bins.
</span>          <span class="c1">// We do this check with log values to prevent overflows in case numCategories is large.
</span>          <span class="c1">// The next check is equivalent to: 2 * ((1 &lt;&lt; numCategories - 1) - 1) &lt;= maxBins
</span>          <span class="k">if</span> <span class="o">(</span><span class="n">numCategories</span> <span class="o">&lt;=</span> <span class="n">maxCategoriesForUnorderedFeature</span><span class="o">)</span> <span class="o">{</span>
            <span class="n">unorderedFeatures</span><span class="o">.</span><span class="n">add</span><span class="o">(</span><span class="n">featureIndex</span><span class="o">)</span>
            <span class="n">numBins</span><span class="o">(</span><span class="n">featureIndex</span><span class="o">)</span> <span class="k">=</span> <span class="n">numUnorderedBins</span><span class="o">(</span><span class="n">numCategories</span><span class="o">)</span>
          <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
            <span class="n">numBins</span><span class="o">(</span><span class="n">featureIndex</span><span class="o">)</span> <span class="k">=</span> <span class="n">numCategories</span>
          <span class="o">}</span>
        <span class="o">}</span>
      <span class="o">}</span>
    <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
      <span class="c1">// 二分类或者回归
</span>      <span class="n">strategy</span><span class="o">.</span><span class="n">categoricalFeaturesInfo</span><span class="o">.</span><span class="n">foreach</span> <span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="n">featureIndex</span><span class="o">,</span> <span class="n">numCategories</span><span class="o">)</span> <span class="k">=&gt;</span>
        <span class="c1">//如果类别特征只有1个类，我们把它看成连续的特征
</span>        <span class="k">if</span> <span class="o">(</span><span class="n">numCategories</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="o">)</span> <span class="o">{</span>
          <span class="n">numBins</span><span class="o">(</span><span class="n">featureIndex</span><span class="o">)</span> <span class="k">=</span> <span class="n">numCategories</span>
        <span class="o">}</span>
      <span class="o">}</span>
    <span class="o">}</span>
    <span class="c1">// 设置每个节点的特征数 (对随机森林而言).
</span>    <span class="k">val</span> <span class="nc">_featureSubsetStrategy</span> <span class="k">=</span> <span class="n">featureSubsetStrategy</span> <span class="k">match</span> <span class="o">{</span>
      <span class="k">case</span> <span class="s">"auto"</span> <span class="k">=&gt;</span>
        <span class="k">if</span> <span class="o">(</span><span class="n">numTrees</span> <span class="o">==</span> <span class="mi">1</span><span class="o">)</span> <span class="o">{</span><span class="c1">//决策树时，使用所有特征
</span>          <span class="s">"all"</span>
        <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
          <span class="k">if</span> <span class="o">(</span><span class="n">strategy</span><span class="o">.</span><span class="n">algo</span> <span class="o">==</span> <span class="nc">Classification</span><span class="o">)</span> <span class="o">{</span><span class="c1">//分类时，使用开平方
</span>            <span class="s">"sqrt"</span>
          <span class="o">}</span> <span class="k">else</span> <span class="o">{</span> <span class="c1">//回归时，使用1/3的特征
</span>            <span class="s">"onethird"</span>
          <span class="o">}</span>
        <span class="o">}</span>
      <span class="k">case</span> <span class="k">_</span> <span class="k">=&gt;</span> <span class="n">featureSubsetStrategy</span>
    <span class="o">}</span>
    <span class="k">val</span> <span class="n">numFeaturesPerNode</span><span class="k">:</span> <span class="kt">Int</span> <span class="o">=</span> <span class="nc">_featureSubsetStrategy</span> <span class="k">match</span> <span class="o">{</span>
      <span class="k">case</span> <span class="s">"all"</span> <span class="k">=&gt;</span> <span class="n">numFeatures</span>
      <span class="k">case</span> <span class="s">"sqrt"</span> <span class="k">=&gt;</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="o">(</span><span class="n">numFeatures</span><span class="o">).</span><span class="n">ceil</span><span class="o">.</span><span class="n">toInt</span>
      <span class="k">case</span> <span class="s">"log2"</span> <span class="k">=&gt;</span> <span class="n">math</span><span class="o">.</span><span class="n">max</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="o">(</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="o">(</span><span class="n">numFeatures</span><span class="o">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="o">(</span><span class="mi">2</span><span class="o">)).</span><span class="n">ceil</span><span class="o">.</span><span class="n">toInt</span><span class="o">)</span>
      <span class="k">case</span> <span class="s">"onethird"</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">numFeatures</span> <span class="o">/</span> <span class="mf">3.0</span><span class="o">).</span><span class="n">ceil</span><span class="o">.</span><span class="n">toInt</span>
    <span class="o">}</span>
    <span class="k">new</span> <span class="nc">DecisionTreeMetadata</span><span class="o">(</span><span class="n">numFeatures</span><span class="o">,</span> <span class="n">numExamples</span><span class="o">,</span> <span class="n">numClasses</span><span class="o">,</span> <span class="n">numBins</span><span class="o">.</span><span class="n">max</span><span class="o">,</span>
      <span class="n">strategy</span><span class="o">.</span><span class="n">categoricalFeaturesInfo</span><span class="o">,</span> <span class="n">unorderedFeatures</span><span class="o">.</span><span class="n">toSet</span><span class="o">,</span> <span class="n">numBins</span><span class="o">,</span>
      <span class="n">strategy</span><span class="o">.</span><span class="n">impurity</span><span class="o">,</span> <span class="n">strategy</span><span class="o">.</span><span class="n">quantileCalculationStrategy</span><span class="o">,</span> <span class="n">strategy</span><span class="o">.</span><span class="n">maxDepth</span><span class="o">,</span>
      <span class="n">strategy</span><span class="o">.</span><span class="n">minInstancesPerNode</span><span class="o">,</span> <span class="n">strategy</span><span class="o">.</span><span class="n">minInfoGain</span><span class="o">,</span> <span class="n">numTrees</span><span class="o">,</span> <span class="n">numFeaturesPerNode</span><span class="o">)</span>
  <span class="o">}</span>
</code></pre>
</div>

<p>  初始化的第二步就是找到切分点（<code class="highlighter-rouge">splits</code>）及箱子信息（<code class="highlighter-rouge">Bins</code>）。这时，调用了<code class="highlighter-rouge">DecisionTree.findSplitsBins</code>方法，进入该方法了解详细信息。</p>

<div class="language-scala highlighter-rouge"><pre class="highlight"><code><span class="cm">/**
   * Returns splits and bins for decision tree calculation.
   * Continuous and categorical features are handled differently.
   *
   * Continuous features:
   *   For each feature, there are numBins - 1 possible splits representing the possible binary
   *   decisions at each node in the tree.
   *   This finds locations (feature values) for splits using a subsample of the data.
   *
   * Categorical features:
   *   For each feature, there is 1 bin per split.
   *   Splits and bins are handled in 2 ways:
   *   (a) "unordered features"
   *       For multiclass classification with a low-arity feature
   *       (i.e., if isMulticlass &amp;&amp; isSpaceSufficientForAllCategoricalSplits),
   *       the feature is split based on subsets of categories.
   *   (b) "ordered features"
   *       For regression and binary classification,
   *       and for multiclass classification with a high-arity feature,
   *       there is one bin per category.
   *
   * @param input Training data: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]]
   * @param metadata Learning and dataset metadata
   * @return A tuple of (splits, bins).
   *         Splits is an Array of [[org.apache.spark.mllib.tree.model.Split]]
   *          of size (numFeatures, numSplits).
   *         Bins is an Array of [[org.apache.spark.mllib.tree.model.Bin]]
   *          of size (numFeatures, numBins).
   */</span>
  <span class="k">protected</span><span class="o">[</span><span class="kt">tree</span><span class="o">]</span> <span class="k">def</span> <span class="n">findSplitsBins</span><span class="o">(</span>
      <span class="n">input</span><span class="k">:</span> <span class="kt">RDD</span><span class="o">[</span><span class="kt">LabeledPoint</span><span class="o">],</span>
      <span class="n">metadata</span><span class="k">:</span> <span class="kt">DecisionTreeMetadata</span><span class="o">)</span><span class="k">:</span> <span class="o">(</span><span class="kt">Array</span><span class="o">[</span><span class="kt">Array</span><span class="o">[</span><span class="kt">Split</span><span class="o">]],</span> <span class="nc">Array</span><span class="o">[</span><span class="kt">Array</span><span class="o">[</span><span class="kt">Bin</span><span class="o">]])</span> <span class="k">=</span> <span class="o">{</span>
    <span class="c1">//特征数
</span>    <span class="k">val</span> <span class="n">numFeatures</span> <span class="k">=</span> <span class="n">metadata</span><span class="o">.</span><span class="n">numFeatures</span>
    <span class="c1">// Sample the input only if there are continuous features.
</span>    <span class="c1">// 判断特征中是否存在连续特征
</span>    <span class="k">val</span> <span class="n">continuousFeatures</span> <span class="k">=</span> <span class="nc">Range</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="n">numFeatures</span><span class="o">).</span><span class="n">filter</span><span class="o">(</span><span class="n">metadata</span><span class="o">.</span><span class="n">isContinuous</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">sampledInput</span> <span class="k">=</span> <span class="k">if</span> <span class="o">(</span><span class="n">continuousFeatures</span><span class="o">.</span><span class="n">nonEmpty</span><span class="o">)</span> <span class="o">{</span>
      <span class="c1">// Calculate the number of samples for approximate quantile calculation.
</span>      <span class="c1">//采样样本数量，最少有 10000 个
</span>      <span class="k">val</span> <span class="n">requiredSamples</span> <span class="k">=</span> <span class="n">math</span><span class="o">.</span><span class="n">max</span><span class="o">(</span><span class="n">metadata</span><span class="o">.</span><span class="n">maxBins</span> <span class="o">*</span> <span class="n">metadata</span><span class="o">.</span><span class="n">maxBins</span><span class="o">,</span> <span class="mi">10000</span><span class="o">)</span>
      <span class="c1">//计算采样比例
</span>      <span class="k">val</span> <span class="n">fraction</span> <span class="k">=</span> <span class="k">if</span> <span class="o">(</span><span class="n">requiredSamples</span> <span class="o">&lt;</span> <span class="n">metadata</span><span class="o">.</span><span class="n">numExamples</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">requiredSamples</span><span class="o">.</span><span class="n">toDouble</span> <span class="o">/</span> <span class="n">metadata</span><span class="o">.</span><span class="n">numExamples</span>
      <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
        <span class="mf">1.0</span>
      <span class="o">}</span>
      <span class="c1">//采样数据，有放回采样
</span>      <span class="n">input</span><span class="o">.</span><span class="n">sample</span><span class="o">(</span><span class="n">withReplacement</span> <span class="k">=</span> <span class="kc">false</span><span class="o">,</span> <span class="n">fraction</span><span class="o">,</span> <span class="k">new</span> <span class="nc">XORShiftRandom</span><span class="o">().</span><span class="n">nextInt</span><span class="o">())</span>
    <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
      <span class="n">input</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">.</span><span class="n">emptyRDD</span><span class="o">[</span><span class="kt">LabeledPoint</span><span class="o">]</span>
    <span class="o">}</span>
    <span class="c1">//分裂点策略，目前 Spark 中只实现了一种策略：排序 Sort
</span>    <span class="n">metadata</span><span class="o">.</span><span class="n">quantileStrategy</span> <span class="k">match</span> <span class="o">{</span>
      <span class="k">case</span> <span class="nc">Sort</span> <span class="k">=&gt;</span>
        <span class="n">findSplitsBinsBySorting</span><span class="o">(</span><span class="n">sampledInput</span><span class="o">,</span> <span class="n">metadata</span><span class="o">,</span> <span class="n">continuousFeatures</span><span class="o">)</span>
      <span class="k">case</span> <span class="nc">MinMax</span> <span class="k">=&gt;</span>
        <span class="k">throw</span> <span class="k">new</span> <span class="nc">UnsupportedOperationException</span><span class="o">(</span><span class="s">"minmax not supported yet."</span><span class="o">)</span>
      <span class="k">case</span> <span class="nc">ApproxHist</span> <span class="k">=&gt;</span>
        <span class="k">throw</span> <span class="k">new</span> <span class="nc">UnsupportedOperationException</span><span class="o">(</span><span class="s">"approximate histogram not supported yet."</span><span class="o">)</span>
    <span class="o">}</span>
  <span class="o">}</span>
</code></pre>
</div>
<p>  我们进入<code class="highlighter-rouge">findSplitsBinsBySorting</code>方法了解<code class="highlighter-rouge">Sort</code>分裂策略的实现。</p>

<div class="language-scala highlighter-rouge"><pre class="highlight"><code><span class="k">private</span> <span class="k">def</span> <span class="n">findSplitsBinsBySorting</span><span class="o">(</span>
      <span class="n">input</span><span class="k">:</span> <span class="kt">RDD</span><span class="o">[</span><span class="kt">LabeledPoint</span><span class="o">],</span>
      <span class="n">metadata</span><span class="k">:</span> <span class="kt">DecisionTreeMetadata</span><span class="o">,</span>
      <span class="n">continuousFeatures</span><span class="k">:</span> <span class="kt">IndexedSeq</span><span class="o">[</span><span class="kt">Int</span><span class="o">])</span><span class="k">:</span> <span class="o">(</span><span class="kt">Array</span><span class="o">[</span><span class="kt">Array</span><span class="o">[</span><span class="kt">Split</span><span class="o">]],</span> <span class="nc">Array</span><span class="o">[</span><span class="kt">Array</span><span class="o">[</span><span class="kt">Bin</span><span class="o">]])</span> <span class="k">=</span> <span class="o">{</span>
    <span class="k">def</span> <span class="n">findSplits</span><span class="o">(</span>
        <span class="n">featureIndex</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span>
        <span class="n">featureSamples</span><span class="k">:</span> <span class="kt">Iterable</span><span class="o">[</span><span class="kt">Double</span><span class="o">])</span><span class="k">:</span> <span class="o">(</span><span class="kt">Int</span><span class="o">,</span> <span class="o">(</span><span class="kt">Array</span><span class="o">[</span><span class="kt">Split</span><span class="o">],</span> <span class="nc">Array</span><span class="o">[</span><span class="kt">Bin</span><span class="o">]))</span> <span class="k">=</span> <span class="o">{</span>
      <span class="c1">//每个特征分别对应一组切分点位置，这里splits是有序的
</span>      <span class="k">val</span> <span class="n">splits</span> <span class="k">=</span> <span class="o">{</span>
        <span class="c1">// findSplitsForContinuousFeature 返回连续特征的所有切分位置
</span>        <span class="k">val</span> <span class="n">featureSplits</span> <span class="k">=</span> <span class="n">findSplitsForContinuousFeature</span><span class="o">(</span>
          <span class="n">featureSamples</span><span class="o">.</span><span class="n">toArray</span><span class="o">,</span>
          <span class="n">metadata</span><span class="o">,</span>
          <span class="n">featureIndex</span><span class="o">)</span>
        <span class="n">featureSplits</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">threshold</span> <span class="k">=&gt;</span> <span class="k">new</span> <span class="nc">Split</span><span class="o">(</span><span class="n">featureIndex</span><span class="o">,</span> <span class="n">threshold</span><span class="o">,</span> <span class="nc">Continuous</span><span class="o">,</span> <span class="nc">Nil</span><span class="o">))</span>
      <span class="o">}</span>
      <span class="c1">//存放切分点位置对应的箱子信息
</span>      <span class="k">val</span> <span class="n">bins</span> <span class="k">=</span> <span class="o">{</span>
        <span class="c1">//采用最小阈值 Double.MinValue 作为最左边的分裂位置并进行装箱
</span>        <span class="k">val</span> <span class="n">lowSplit</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DummyLowSplit</span><span class="o">(</span><span class="n">featureIndex</span><span class="o">,</span> <span class="nc">Continuous</span><span class="o">)</span>
        <span class="c1">//最后一个箱子的计算采用最大阈值 Double.MaxValue 作为最右边的切分位置
</span>        <span class="k">val</span> <span class="n">highSplit</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DummyHighSplit</span><span class="o">(</span><span class="n">featureIndex</span><span class="o">,</span> <span class="nc">Continuous</span><span class="o">)</span>
        <span class="c1">// tack the dummy splits on either side of the computed splits
</span>        <span class="k">val</span> <span class="n">allSplits</span> <span class="k">=</span> <span class="n">lowSplit</span> <span class="o">+:</span> <span class="n">splits</span><span class="o">.</span><span class="n">toSeq</span> <span class="o">:+</span> <span class="n">highSplit</span>
        <span class="c1">//将切分点两两结合成一个箱子
</span>        <span class="n">allSplits</span><span class="o">.</span><span class="n">sliding</span><span class="o">(</span><span class="mi">2</span><span class="o">).</span><span class="n">map</span> <span class="o">{</span>
          <span class="k">case</span> <span class="nc">Seq</span><span class="o">(</span><span class="n">left</span><span class="o">,</span> <span class="n">right</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="k">new</span> <span class="nc">Bin</span><span class="o">(</span><span class="n">left</span><span class="o">,</span> <span class="n">right</span><span class="o">,</span> <span class="nc">Continuous</span><span class="o">,</span> <span class="nc">Double</span><span class="o">.</span><span class="nc">MinValue</span><span class="o">)</span>
        <span class="o">}.</span><span class="n">toArray</span>
      <span class="o">}</span>
      <span class="o">(</span><span class="n">featureIndex</span><span class="o">,</span> <span class="o">(</span><span class="n">splits</span><span class="o">,</span> <span class="n">bins</span><span class="o">))</span>
    <span class="o">}</span>
    <span class="k">val</span> <span class="n">continuousSplits</span> <span class="k">=</span> <span class="o">{</span>
      <span class="c1">// reduce the parallelism for split computations when there are less
</span>      <span class="c1">// continuous features than input partitions. this prevents tasks from
</span>      <span class="c1">// being spun up that will definitely do no work.
</span>      <span class="k">val</span> <span class="n">numPartitions</span> <span class="k">=</span> <span class="n">math</span><span class="o">.</span><span class="n">min</span><span class="o">(</span><span class="n">continuousFeatures</span><span class="o">.</span><span class="n">length</span><span class="o">,</span> <span class="n">input</span><span class="o">.</span><span class="n">partitions</span><span class="o">.</span><span class="n">length</span><span class="o">)</span>
      <span class="n">input</span>
        <span class="o">.</span><span class="n">flatMap</span><span class="o">(</span><span class="n">point</span> <span class="k">=&gt;</span> <span class="n">continuousFeatures</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">idx</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">idx</span><span class="o">,</span> <span class="n">point</span><span class="o">.</span><span class="n">features</span><span class="o">(</span><span class="n">idx</span><span class="o">))))</span>
        <span class="o">.</span><span class="n">groupByKey</span><span class="o">(</span><span class="n">numPartitions</span><span class="o">)</span>
        <span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="n">k</span><span class="o">,</span> <span class="n">v</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">findSplits</span><span class="o">(</span><span class="n">k</span><span class="o">,</span> <span class="n">v</span><span class="o">)</span> <span class="o">}</span>
        <span class="o">.</span><span class="n">collectAsMap</span><span class="o">()</span>
    <span class="o">}</span>
    <span class="k">val</span> <span class="n">numFeatures</span> <span class="k">=</span> <span class="n">metadata</span><span class="o">.</span><span class="n">numFeatures</span>
    <span class="c1">//遍历所有特征
</span>    <span class="k">val</span> <span class="o">(</span><span class="n">splits</span><span class="o">,</span> <span class="n">bins</span><span class="o">)</span> <span class="k">=</span> <span class="nc">Range</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="n">numFeatures</span><span class="o">).</span><span class="n">unzip</span> <span class="o">{</span>
      <span class="c1">//处理连续特征的情况
</span>      <span class="k">case</span> <span class="n">i</span> <span class="k">if</span> <span class="n">metadata</span><span class="o">.</span><span class="n">isContinuous</span><span class="o">(</span><span class="n">i</span><span class="o">)</span> <span class="k">=&gt;</span>
        <span class="k">val</span> <span class="o">(</span><span class="n">split</span><span class="o">,</span> <span class="n">bin</span><span class="o">)</span> <span class="k">=</span> <span class="n">continuousSplits</span><span class="o">(</span><span class="n">i</span><span class="o">)</span>
        <span class="n">metadata</span><span class="o">.</span><span class="n">setNumSplits</span><span class="o">(</span><span class="n">i</span><span class="o">,</span> <span class="n">split</span><span class="o">.</span><span class="n">length</span><span class="o">)</span>
        <span class="o">(</span><span class="n">split</span><span class="o">,</span> <span class="n">bin</span><span class="o">)</span>
      <span class="c1">//处理离散特征且无序的情况
</span>      <span class="k">case</span> <span class="n">i</span> <span class="k">if</span> <span class="n">metadata</span><span class="o">.</span><span class="n">isCategorical</span><span class="o">(</span><span class="n">i</span><span class="o">)</span> <span class="o">&amp;&amp;</span> <span class="n">metadata</span><span class="o">.</span><span class="n">isUnordered</span><span class="o">(</span><span class="n">i</span><span class="o">)</span> <span class="k">=&gt;</span>
        <span class="c1">// Unordered features
</span>        <span class="c1">// 2^(maxFeatureValue - 1) - 1 combinations
</span>        <span class="k">val</span> <span class="n">featureArity</span> <span class="k">=</span> <span class="n">metadata</span><span class="o">.</span><span class="n">featureArity</span><span class="o">(</span><span class="n">i</span><span class="o">)</span>
        <span class="k">val</span> <span class="n">split</span> <span class="k">=</span> <span class="nc">Range</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="n">metadata</span><span class="o">.</span><span class="n">numSplits</span><span class="o">(</span><span class="n">i</span><span class="o">)).</span><span class="n">map</span> <span class="o">{</span> <span class="n">splitIndex</span> <span class="k">=&gt;</span>
          <span class="k">val</span> <span class="n">categories</span> <span class="k">=</span> <span class="n">extractMultiClassCategories</span><span class="o">(</span><span class="n">splitIndex</span> <span class="o">+</span> <span class="mi">1</span><span class="o">,</span> <span class="n">featureArity</span><span class="o">)</span>
          <span class="k">new</span> <span class="nc">Split</span><span class="o">(</span><span class="n">i</span><span class="o">,</span> <span class="nc">Double</span><span class="o">.</span><span class="nc">MinValue</span><span class="o">,</span> <span class="nc">Categorical</span><span class="o">,</span> <span class="n">categories</span><span class="o">)</span>
        <span class="o">}</span>
        <span class="c1">// For unordered categorical features, there is no need to construct the bins.
</span>        <span class="c1">// since there is a one-to-one correspondence between the splits and the bins.
</span>        <span class="o">(</span><span class="n">split</span><span class="o">.</span><span class="n">toArray</span><span class="o">,</span> <span class="nc">Array</span><span class="o">.</span><span class="n">empty</span><span class="o">[</span><span class="kt">Bin</span><span class="o">])</span>
      <span class="c1">//处理离散特征且有序的情况
</span>      <span class="k">case</span> <span class="n">i</span> <span class="k">if</span> <span class="n">metadata</span><span class="o">.</span><span class="n">isCategorical</span><span class="o">(</span><span class="n">i</span><span class="o">)</span> <span class="k">=&gt;</span>
        <span class="c1">//有序特征无需处理，箱子与特征值对应
</span>        <span class="c1">// Ordered features
</span>        <span class="c1">// Bins correspond to feature values, so we do not need to compute splits or bins
</span>        <span class="c1">// beforehand.  Splits are constructed as needed during training.
</span>        <span class="o">(</span><span class="nc">Array</span><span class="o">.</span><span class="n">empty</span><span class="o">[</span><span class="kt">Split</span><span class="o">],</span> <span class="nc">Array</span><span class="o">.</span><span class="n">empty</span><span class="o">[</span><span class="kt">Bin</span><span class="o">])</span>
    <span class="o">}</span>
    <span class="o">(</span><span class="n">splits</span><span class="o">.</span><span class="n">toArray</span><span class="o">,</span> <span class="n">bins</span><span class="o">.</span><span class="n">toArray</span><span class="o">)</span>
  <span class="o">}</span>
</code></pre>
</div>

<p>  计算连续特征的所有切分位置需要调用方法<code class="highlighter-rouge">findSplitsForContinuousFeature</code>方法。</p>

<div class="language-scala highlighter-rouge"><pre class="highlight"><code><span class="k">private</span><span class="o">[</span><span class="kt">tree</span><span class="o">]</span> <span class="k">def</span> <span class="n">findSplitsForContinuousFeature</span><span class="o">(</span>
      <span class="n">featureSamples</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span class="kt">Double</span><span class="o">],</span>
      <span class="n">metadata</span><span class="k">:</span> <span class="kt">DecisionTreeMetadata</span><span class="o">,</span>
      <span class="n">featureIndex</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span class="kt">Double</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
    <span class="k">val</span> <span class="n">splits</span> <span class="k">=</span> <span class="o">{</span>
      <span class="c1">//切分数是bin的数量减1，即m-1
</span>      <span class="k">val</span> <span class="n">numSplits</span> <span class="k">=</span> <span class="n">metadata</span><span class="o">.</span><span class="n">numSplits</span><span class="o">(</span><span class="n">featureIndex</span><span class="o">)</span>
      <span class="c1">// （特征，特征出现的次数）
</span>      <span class="k">val</span> <span class="n">valueCountMap</span> <span class="k">=</span> <span class="n">featureSamples</span><span class="o">.</span><span class="n">foldLeft</span><span class="o">(</span><span class="nc">Map</span><span class="o">.</span><span class="n">empty</span><span class="o">[</span><span class="kt">Double</span>, <span class="kt">Int</span><span class="o">])</span> <span class="o">{</span> <span class="o">(</span><span class="n">m</span><span class="o">,</span> <span class="n">x</span><span class="o">)</span> <span class="k">=&gt;</span>
        <span class="n">m</span> <span class="o">+</span> <span class="o">((</span><span class="n">x</span><span class="o">,</span> <span class="n">m</span><span class="o">.</span><span class="n">getOrElse</span><span class="o">(</span><span class="n">x</span><span class="o">,</span> <span class="mi">0</span><span class="o">)</span> <span class="o">+</span> <span class="mi">1</span><span class="o">))</span>
      <span class="o">}</span>
      <span class="c1">// 根据特征进行排序
</span>      <span class="k">val</span> <span class="n">valueCounts</span> <span class="k">=</span> <span class="n">valueCountMap</span><span class="o">.</span><span class="n">toSeq</span><span class="o">.</span><span class="n">sortBy</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">_1</span><span class="o">).</span><span class="n">toArray</span>
      <span class="c1">// if possible splits is not enough or just enough, just return all possible splits
</span>      <span class="k">val</span> <span class="n">possibleSplits</span> <span class="k">=</span> <span class="n">valueCounts</span><span class="o">.</span><span class="n">length</span>
      <span class="c1">//如果特征数小于切分数，所有特征均作为切分点
</span>      <span class="k">if</span> <span class="o">(</span><span class="n">possibleSplits</span> <span class="o">&lt;=</span> <span class="n">numSplits</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">valueCounts</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">_1</span><span class="o">)</span>
      <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
        <span class="c1">// 切分点之间的步长
</span>        <span class="k">val</span> <span class="n">stride</span><span class="k">:</span> <span class="kt">Double</span> <span class="o">=</span> <span class="n">featureSamples</span><span class="o">.</span><span class="n">length</span><span class="o">.</span><span class="n">toDouble</span> <span class="o">/</span> <span class="o">(</span><span class="n">numSplits</span> <span class="o">+</span> <span class="mi">1</span><span class="o">)</span>
        <span class="k">val</span> <span class="n">splitsBuilder</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">.</span><span class="n">newBuilder</span><span class="o">[</span><span class="kt">Double</span><span class="o">]</span>
        <span class="k">var</span> <span class="n">index</span> <span class="k">=</span> <span class="mi">1</span>
        <span class="c1">// currentCount: sum of counts of values that have been visited
</span>        <span class="c1">//第一个特征的出现次数
</span>        <span class="k">var</span> <span class="n">currentCount</span> <span class="k">=</span> <span class="n">valueCounts</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">_2</span>
        <span class="c1">// targetCount: target value for `currentCount`.
</span>        <span class="c1">// If `currentCount` is closest value to `targetCount`,
</span>        <span class="c1">// then current value is a split threshold.
</span>        <span class="c1">// After finding a split threshold, `targetCount` is added by stride.
</span>        <span class="c1">// 如果currentCount离targetCount最近，那么当前值是切分点
</span>        <span class="k">var</span> <span class="n">targetCount</span> <span class="k">=</span> <span class="n">stride</span>
        <span class="k">while</span> <span class="o">(</span><span class="n">index</span> <span class="o">&lt;</span> <span class="n">valueCounts</span><span class="o">.</span><span class="n">length</span><span class="o">)</span> <span class="o">{</span>
          <span class="k">val</span> <span class="n">previousCount</span> <span class="k">=</span> <span class="n">currentCount</span>
          <span class="n">currentCount</span> <span class="o">+=</span> <span class="n">valueCounts</span><span class="o">(</span><span class="n">index</span><span class="o">).</span><span class="n">_2</span>
          <span class="k">val</span> <span class="n">previousGap</span> <span class="k">=</span> <span class="n">math</span><span class="o">.</span><span class="n">abs</span><span class="o">(</span><span class="n">previousCount</span> <span class="o">-</span> <span class="n">targetCount</span><span class="o">)</span>
          <span class="k">val</span> <span class="n">currentGap</span> <span class="k">=</span> <span class="n">math</span><span class="o">.</span><span class="n">abs</span><span class="o">(</span><span class="n">currentCount</span> <span class="o">-</span> <span class="n">targetCount</span><span class="o">)</span>
          <span class="c1">// If adding count of current value to currentCount
</span>          <span class="c1">// makes the gap between currentCount and targetCount smaller,
</span>          <span class="c1">// previous value is a split threshold.
</span>          <span class="k">if</span> <span class="o">(</span><span class="n">previousGap</span> <span class="o">&lt;</span> <span class="n">currentGap</span><span class="o">)</span> <span class="o">{</span>
            <span class="n">splitsBuilder</span> <span class="o">+=</span> <span class="n">valueCounts</span><span class="o">(</span><span class="n">index</span> <span class="o">-</span> <span class="mi">1</span><span class="o">).</span><span class="n">_1</span>
            <span class="n">targetCount</span> <span class="o">+=</span> <span class="n">stride</span>
          <span class="o">}</span>
          <span class="n">index</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="o">}</span>
        <span class="n">splitsBuilder</span><span class="o">.</span><span class="n">result</span><span class="o">()</span>
      <span class="o">}</span>
    <span class="o">}</span>
    <span class="n">splits</span>
  <span class="o">}</span>
</code></pre>
</div>

<h4 id="section-7">5.1.2 迭代构建随机森林</h4>

<div class="language-scala highlighter-rouge"><pre class="highlight"><code><span class="c1">//节点是否使用缓存，节点 ID 从 1 开始，1 即为这颗树的根节点，左节点为 2，右节点为 3，依次递增下去
</span><span class="k">val</span> <span class="n">nodeIdCache</span> <span class="k">=</span> <span class="k">if</span> <span class="o">(</span><span class="n">strategy</span><span class="o">.</span><span class="n">useNodeIdCache</span><span class="o">)</span> <span class="o">{</span>
   <span class="nc">Some</span><span class="o">(</span><span class="nc">NodeIdCache</span><span class="o">.</span><span class="n">init</span><span class="o">(</span>
        <span class="n">data</span> <span class="k">=</span> <span class="n">baggedInput</span><span class="o">,</span>
        <span class="n">numTrees</span> <span class="k">=</span> <span class="n">numTrees</span><span class="o">,</span>
        <span class="n">checkpointInterval</span> <span class="k">=</span> <span class="n">strategy</span><span class="o">.</span><span class="n">checkpointInterval</span><span class="o">,</span>
        <span class="n">initVal</span> <span class="k">=</span> <span class="mi">1</span><span class="o">))</span>
<span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
   <span class="nc">None</span>
<span class="o">}</span>
<span class="c1">// FIFO queue of nodes to train: (treeIndex, node)
</span><span class="k">val</span> <span class="n">nodeQueue</span> <span class="k">=</span> <span class="k">new</span> <span class="n">mutable</span><span class="o">.</span><span class="nc">Queue</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">Node</span><span class="o">)]()</span>
<span class="k">val</span> <span class="n">rng</span> <span class="k">=</span> <span class="k">new</span> <span class="n">scala</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="nc">Random</span><span class="o">()</span>
<span class="n">rng</span><span class="o">.</span><span class="n">setSeed</span><span class="o">(</span><span class="n">seed</span><span class="o">)</span>
<span class="c1">// Allocate and queue root nodes.
//创建树的根节点
</span><span class="k">val</span> <span class="n">topNodes</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span class="kt">Node</span><span class="o">]</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">.</span><span class="n">fill</span><span class="o">[</span><span class="kt">Node</span><span class="o">](</span><span class="n">numTrees</span><span class="o">)(</span><span class="nc">Node</span><span class="o">.</span><span class="n">emptyNode</span><span class="o">(</span><span class="n">nodeIndex</span> <span class="k">=</span> <span class="mi">1</span><span class="o">))</span>
<span class="c1">//将（树的索引，树的根节点）入队，树索引从 0 开始，根节点从 1 开始
</span><span class="nc">Range</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="n">numTrees</span><span class="o">).</span><span class="n">foreach</span><span class="o">(</span><span class="n">treeIndex</span> <span class="k">=&gt;</span> <span class="n">nodeQueue</span><span class="o">.</span><span class="n">enqueue</span><span class="o">((</span><span class="n">treeIndex</span><span class="o">,</span> <span class="n">topNodes</span><span class="o">(</span><span class="n">treeIndex</span><span class="o">))))</span>
<span class="k">while</span> <span class="o">(</span><span class="n">nodeQueue</span><span class="o">.</span><span class="n">nonEmpty</span><span class="o">)</span> <span class="o">{</span>
    <span class="c1">// Collect some nodes to split, and choose features for each node (if subsampling).
</span>    <span class="c1">// Each group of nodes may come from one or multiple trees, and at multiple levels.
</span>    <span class="c1">// 取得每个树所有需要切分的节点,nodesForGroup表示需要切分的节点
</span>    <span class="k">val</span> <span class="o">(</span><span class="n">nodesForGroup</span><span class="o">,</span> <span class="n">treeToNodeToIndexInfo</span><span class="o">)</span> <span class="k">=</span>
        <span class="nc">RandomForest</span><span class="o">.</span><span class="n">selectNodesToSplit</span><span class="o">(</span><span class="n">nodeQueue</span><span class="o">,</span> <span class="n">maxMemoryUsage</span><span class="o">,</span> <span class="n">metadata</span><span class="o">,</span> <span class="n">rng</span><span class="o">)</span>
    <span class="c1">//找出最优切点
</span>    <span class="nc">DecisionTree</span><span class="o">.</span><span class="n">findBestSplits</span><span class="o">(</span><span class="n">baggedInput</span><span class="o">,</span> <span class="n">metadata</span><span class="o">,</span> <span class="n">topNodes</span><span class="o">,</span> <span class="n">nodesForGroup</span><span class="o">,</span>
        <span class="n">treeToNodeToIndexInfo</span><span class="o">,</span> <span class="n">splits</span><span class="o">,</span> <span class="n">bins</span><span class="o">,</span> <span class="n">nodeQueue</span><span class="o">,</span> <span class="n">timer</span><span class="o">,</span> <span class="n">nodeIdCache</span> <span class="k">=</span> <span class="n">nodeIdCache</span><span class="o">)</span>
<span class="o">}</span>
</code></pre>
</div>

<p>  这里有两点需要重点介绍，第一点是取得每个树所有需要切分的节点，通过<code class="highlighter-rouge">RandomForest.selectNodesToSplit</code>方法实现；第二点是找出最优的切分，通过<code class="highlighter-rouge">DecisionTree.findBestSplits</code>方法实现。下面分别介绍这两点。</p>

<ul>
  <li>取得每个树所有需要切分的节点</li>
</ul>

<div class="language-scala highlighter-rouge"><pre class="highlight"><code> <span class="k">private</span><span class="o">[</span><span class="kt">tree</span><span class="o">]</span> <span class="k">def</span> <span class="n">selectNodesToSplit</span><span class="o">(</span>
      <span class="n">nodeQueue</span><span class="k">:</span> <span class="kt">mutable.Queue</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">Node</span><span class="o">)],</span>
      <span class="n">maxMemoryUsage</span><span class="k">:</span> <span class="kt">Long</span><span class="o">,</span>
      <span class="n">metadata</span><span class="k">:</span> <span class="kt">DecisionTreeMetadata</span><span class="o">,</span>
      <span class="n">rng</span><span class="k">:</span> <span class="kt">scala.util.Random</span><span class="o">)</span><span class="k">:</span> <span class="o">(</span><span class="kt">Map</span><span class="o">[</span><span class="kt">Int</span>, <span class="kt">Array</span><span class="o">[</span><span class="kt">Node</span><span class="o">]],</span> <span class="nc">Map</span><span class="o">[</span><span class="kt">Int</span>, <span class="kt">Map</span><span class="o">[</span><span class="kt">Int</span>, <span class="kt">NodeIndexInfo</span><span class="o">]])</span> <span class="k">=</span> <span class="o">{</span>
    <span class="c1">// nodesForGroup保存需要切分的节点，treeIndex --&gt; nodes
</span>    <span class="k">val</span> <span class="n">mutableNodesForGroup</span> <span class="k">=</span> <span class="k">new</span> <span class="n">mutable</span><span class="o">.</span><span class="nc">HashMap</span><span class="o">[</span><span class="kt">Int</span>, <span class="kt">mutable.ArrayBuffer</span><span class="o">[</span><span class="kt">Node</span><span class="o">]]()</span>
    <span class="c1">// mutableTreeToNodeToIndexInfo保存每个节点中选中特征的索引
</span>    <span class="c1">// treeIndex --&gt; (global) node index --&gt; (node index in group, feature indices)
</span>    <span class="c1">//(global) node index是树中的索引，组中节点索引的范围是[0, numNodesInGroup)
</span>    <span class="k">val</span> <span class="n">mutableTreeToNodeToIndexInfo</span> <span class="k">=</span>
      <span class="k">new</span> <span class="n">mutable</span><span class="o">.</span><span class="nc">HashMap</span><span class="o">[</span><span class="kt">Int</span>, <span class="kt">mutable.HashMap</span><span class="o">[</span><span class="kt">Int</span>, <span class="kt">NodeIndexInfo</span><span class="o">]]()</span>
    <span class="k">var</span> <span class="n">memUsage</span><span class="k">:</span> <span class="kt">Long</span> <span class="o">=</span> <span class="mi">0L</span>
    <span class="k">var</span> <span class="n">numNodesInGroup</span> <span class="k">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="o">(</span><span class="n">nodeQueue</span><span class="o">.</span><span class="n">nonEmpty</span> <span class="o">&amp;&amp;</span> <span class="n">memUsage</span> <span class="o">&lt;</span> <span class="n">maxMemoryUsage</span><span class="o">)</span> <span class="o">{</span>
      <span class="k">val</span> <span class="o">(</span><span class="n">treeIndex</span><span class="o">,</span> <span class="n">node</span><span class="o">)</span> <span class="k">=</span> <span class="n">nodeQueue</span><span class="o">.</span><span class="n">head</span>
      <span class="c1">// Choose subset of features for node (if subsampling).
</span>      <span class="c1">// 选中特征子集
</span>      <span class="k">val</span> <span class="n">featureSubset</span><span class="k">:</span> <span class="kt">Option</span><span class="o">[</span><span class="kt">Array</span><span class="o">[</span><span class="kt">Int</span><span class="o">]]</span> <span class="k">=</span> <span class="k">if</span> <span class="o">(</span><span class="n">metadata</span><span class="o">.</span><span class="n">subsamplingFeatures</span><span class="o">)</span> <span class="o">{</span>
        <span class="nc">Some</span><span class="o">(</span><span class="nc">SamplingUtils</span><span class="o">.</span><span class="n">reservoirSampleAndCount</span><span class="o">(</span><span class="nc">Range</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span>
          <span class="n">metadata</span><span class="o">.</span><span class="n">numFeatures</span><span class="o">).</span><span class="n">iterator</span><span class="o">,</span> <span class="n">metadata</span><span class="o">.</span><span class="n">numFeaturesPerNode</span><span class="o">,</span> <span class="n">rng</span><span class="o">.</span><span class="n">nextLong</span><span class="o">).</span><span class="n">_1</span><span class="o">)</span>
      <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
        <span class="nc">None</span>
      <span class="o">}</span>
      <span class="c1">// Check if enough memory remains to add this node to the group.
</span>      <span class="c1">// 检查是否有足够的内存
</span>      <span class="k">val</span> <span class="n">nodeMemUsage</span> <span class="k">=</span> <span class="nc">RandomForest</span><span class="o">.</span><span class="n">aggregateSizeForNode</span><span class="o">(</span><span class="n">metadata</span><span class="o">,</span> <span class="n">featureSubset</span><span class="o">)</span> <span class="o">*</span> <span class="mi">8L</span>
      <span class="k">if</span> <span class="o">(</span><span class="n">memUsage</span> <span class="o">+</span> <span class="n">nodeMemUsage</span> <span class="o">&lt;=</span> <span class="n">maxMemoryUsage</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">nodeQueue</span><span class="o">.</span><span class="n">dequeue</span><span class="o">()</span>
        <span class="n">mutableNodesForGroup</span><span class="o">.</span><span class="n">getOrElseUpdate</span><span class="o">(</span><span class="n">treeIndex</span><span class="o">,</span> <span class="k">new</span> <span class="n">mutable</span><span class="o">.</span><span class="nc">ArrayBuffer</span><span class="o">[</span><span class="kt">Node</span><span class="o">]())</span> <span class="o">+=</span> <span class="n">node</span>
        <span class="n">mutableTreeToNodeToIndexInfo</span>
          <span class="o">.</span><span class="n">getOrElseUpdate</span><span class="o">(</span><span class="n">treeIndex</span><span class="o">,</span> <span class="k">new</span> <span class="n">mutable</span><span class="o">.</span><span class="nc">HashMap</span><span class="o">[</span><span class="kt">Int</span>, <span class="kt">NodeIndexInfo</span><span class="o">]())(</span><span class="n">node</span><span class="o">.</span><span class="n">id</span><span class="o">)</span>
          <span class="k">=</span> <span class="k">new</span> <span class="nc">NodeIndexInfo</span><span class="o">(</span><span class="n">numNodesInGroup</span><span class="o">,</span> <span class="n">featureSubset</span><span class="o">)</span>
      <span class="o">}</span>
      <span class="n">numNodesInGroup</span> <span class="o">+=</span> <span class="mi">1</span>
      <span class="n">memUsage</span> <span class="o">+=</span> <span class="n">nodeMemUsage</span>
    <span class="o">}</span>
    <span class="c1">// 将可变map转换为不可变map
</span>    <span class="k">val</span> <span class="n">nodesForGroup</span><span class="k">:</span> <span class="kt">Map</span><span class="o">[</span><span class="kt">Int</span>, <span class="kt">Array</span><span class="o">[</span><span class="kt">Node</span><span class="o">]]</span> <span class="k">=</span> <span class="n">mutableNodesForGroup</span><span class="o">.</span><span class="n">mapValues</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">toArray</span><span class="o">).</span><span class="n">toMap</span>
    <span class="k">val</span> <span class="n">treeToNodeToIndexInfo</span> <span class="k">=</span> <span class="n">mutableTreeToNodeToIndexInfo</span><span class="o">.</span><span class="n">mapValues</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">toMap</span><span class="o">).</span><span class="n">toMap</span>
    <span class="o">(</span><span class="n">nodesForGroup</span><span class="o">,</span> <span class="n">treeToNodeToIndexInfo</span><span class="o">)</span>
  <span class="o">}</span>
</code></pre>
</div>

<ul>
  <li>选中最优切分</li>
</ul>

<div class="language-scala highlighter-rouge"><pre class="highlight"><code><span class="c1">//所有可切分的节点
</span><span class="k">val</span> <span class="n">nodes</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Array</span><span class="o">[</span><span class="kt">Node</span><span class="o">](</span><span class="n">numNodes</span><span class="o">)</span>
<span class="n">nodesForGroup</span><span class="o">.</span><span class="n">foreach</span> <span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="n">treeIndex</span><span class="o">,</span> <span class="n">nodesForTree</span><span class="o">)</span> <span class="k">=&gt;</span>
   <span class="n">nodesForTree</span><span class="o">.</span><span class="n">foreach</span> <span class="o">{</span> <span class="n">node</span> <span class="k">=&gt;</span>
     <span class="n">nodes</span><span class="o">(</span><span class="n">treeToNodeToIndexInfo</span><span class="o">(</span><span class="n">treeIndex</span><span class="o">)(</span><span class="n">node</span><span class="o">.</span><span class="n">id</span><span class="o">).</span><span class="n">nodeIndexInGroup</span><span class="o">)</span> <span class="k">=</span> <span class="n">node</span>
   <span class="o">}</span>
<span class="o">}</span>
<span class="c1">// In each partition, iterate all instances and compute aggregate stats for each node,
// yield an (nodeIndex, nodeAggregateStats) pair for each node.
// After a `reduceByKey` operation,
// stats of a node will be shuffled to a particular partition and be combined together,
// then best splits for nodes are found there.
// Finally, only best Splits for nodes are collected to driver to construct decision tree.
//获取节点对应的特征
</span><span class="k">val</span> <span class="n">nodeToFeatures</span> <span class="k">=</span> <span class="n">getNodeToFeatures</span><span class="o">(</span><span class="n">treeToNodeToIndexInfo</span><span class="o">)</span>
<span class="k">val</span> <span class="n">nodeToFeaturesBc</span> <span class="k">=</span> <span class="n">input</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">.</span><span class="n">broadcast</span><span class="o">(</span><span class="n">nodeToFeatures</span><span class="o">)</span>
<span class="k">val</span> <span class="n">partitionAggregates</span> <span class="k">:</span> <span class="kt">RDD</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">DTStatsAggregator</span><span class="o">)]</span> <span class="k">=</span> <span class="k">if</span> <span class="o">(</span><span class="n">nodeIdCache</span><span class="o">.</span><span class="n">nonEmpty</span><span class="o">)</span> <span class="o">{</span>
    <span class="n">input</span><span class="o">.</span><span class="n">zip</span><span class="o">(</span><span class="n">nodeIdCache</span><span class="o">.</span><span class="n">get</span><span class="o">.</span><span class="n">nodeIdsForInstances</span><span class="o">).</span><span class="n">mapPartitions</span> <span class="o">{</span> <span class="n">points</span> <span class="k">=&gt;</span>
      <span class="c1">// Construct a nodeStatsAggregators array to hold node aggregate stats,
</span>      <span class="c1">// each node will have a nodeStatsAggregator
</span>      <span class="k">val</span> <span class="n">nodeStatsAggregators</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">.</span><span class="n">tabulate</span><span class="o">(</span><span class="n">numNodes</span><span class="o">)</span> <span class="o">{</span> <span class="n">nodeIndex</span> <span class="k">=&gt;</span>
          <span class="c1">//节点对应的特征集
</span>          <span class="k">val</span> <span class="n">featuresForNode</span> <span class="k">=</span> <span class="n">nodeToFeaturesBc</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">flatMap</span> <span class="o">{</span> <span class="n">nodeToFeatures</span> <span class="k">=&gt;</span>
            <span class="nc">Some</span><span class="o">(</span><span class="n">nodeToFeatures</span><span class="o">(</span><span class="n">nodeIndex</span><span class="o">))</span>
          <span class="o">}</span>
          <span class="c1">// DTStatsAggregator，其中引用了 ImpurityAggregator，给出计算不纯度 impurity 的逻辑
</span>          <span class="k">new</span> <span class="nc">DTStatsAggregator</span><span class="o">(</span><span class="n">metadata</span><span class="o">,</span> <span class="n">featuresForNode</span><span class="o">)</span>
      <span class="o">}</span>
      <span class="c1">// 迭代当前分区的所有对象，更新聚合统计信息，统计信息即采样数据的权重值
</span>      <span class="n">points</span><span class="o">.</span><span class="n">foreach</span><span class="o">(</span><span class="n">binSeqOpWithNodeIdCache</span><span class="o">(</span><span class="n">nodeStatsAggregators</span><span class="o">,</span> <span class="k">_</span><span class="o">))</span>
      <span class="c1">// transform nodeStatsAggregators array to (nodeIndex, nodeAggregateStats) pairs,
</span>      <span class="c1">// which can be combined with other partition using `reduceByKey`
</span>      <span class="n">nodeStatsAggregators</span><span class="o">.</span><span class="n">view</span><span class="o">.</span><span class="n">zipWithIndex</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">swap</span><span class="o">).</span><span class="n">iterator</span>
    <span class="o">}</span>
<span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
      <span class="n">input</span><span class="o">.</span><span class="n">mapPartitions</span> <span class="o">{</span> <span class="n">points</span> <span class="k">=&gt;</span>
        <span class="c1">// Construct a nodeStatsAggregators array to hold node aggregate stats,
</span>        <span class="c1">// each node will have a nodeStatsAggregator
</span>        <span class="k">val</span> <span class="n">nodeStatsAggregators</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">.</span><span class="n">tabulate</span><span class="o">(</span><span class="n">numNodes</span><span class="o">)</span> <span class="o">{</span> <span class="n">nodeIndex</span> <span class="k">=&gt;</span>
          <span class="c1">//节点对应的特征集
</span>          <span class="k">val</span> <span class="n">featuresForNode</span> <span class="k">=</span> <span class="n">nodeToFeaturesBc</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">flatMap</span> <span class="o">{</span> <span class="n">nodeToFeatures</span> <span class="k">=&gt;</span>
            <span class="nc">Some</span><span class="o">(</span><span class="n">nodeToFeatures</span><span class="o">(</span><span class="n">nodeIndex</span><span class="o">))</span>
          <span class="o">}</span>
          <span class="c1">// DTStatsAggregator，其中引用了 ImpurityAggregator，给出计算不纯度 impurity 的逻辑
</span>          <span class="k">new</span> <span class="nc">DTStatsAggregator</span><span class="o">(</span><span class="n">metadata</span><span class="o">,</span> <span class="n">featuresForNode</span><span class="o">)</span>
        <span class="o">}</span>
        <span class="c1">// 迭代当前分区的所有对象，更新聚合统计信息
</span>        <span class="n">points</span><span class="o">.</span><span class="n">foreach</span><span class="o">(</span><span class="n">binSeqOp</span><span class="o">(</span><span class="n">nodeStatsAggregators</span><span class="o">,</span> <span class="k">_</span><span class="o">))</span>
        <span class="c1">// transform nodeStatsAggregators array to (nodeIndex, nodeAggregateStats) pairs,
</span>        <span class="c1">// which can be combined with other partition using `reduceByKey`
</span>        <span class="n">nodeStatsAggregators</span><span class="o">.</span><span class="n">view</span><span class="o">.</span><span class="n">zipWithIndex</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">swap</span><span class="o">).</span><span class="n">iterator</span>
      <span class="o">}</span>
<span class="o">}</span>
<span class="k">val</span> <span class="n">nodeToBestSplits</span> <span class="k">=</span> <span class="n">partitionAggregates</span><span class="o">.</span><span class="n">reduceByKey</span><span class="o">((</span><span class="n">a</span><span class="o">,</span> <span class="n">b</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">a</span><span class="o">.</span><span class="n">merge</span><span class="o">(</span><span class="n">b</span><span class="o">))</span>
    <span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="n">nodeIndex</span><span class="o">,</span> <span class="n">aggStats</span><span class="o">)</span> <span class="k">=&gt;</span>
          <span class="k">val</span> <span class="n">featuresForNode</span> <span class="k">=</span> <span class="n">nodeToFeaturesBc</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="n">nodeToFeatures</span> <span class="k">=&gt;</span>
            <span class="n">nodeToFeatures</span><span class="o">(</span><span class="n">nodeIndex</span><span class="o">)</span>
    <span class="o">}</span>
    <span class="c1">// find best split for each node
</span>    <span class="k">val</span> <span class="o">(</span><span class="n">split</span><span class="k">:</span> <span class="kt">Split</span><span class="o">,</span> <span class="n">stats</span><span class="k">:</span> <span class="kt">InformationGainStats</span><span class="o">,</span> <span class="n">predict</span><span class="k">:</span> <span class="kt">Predict</span><span class="o">)</span> <span class="k">=</span>
        <span class="n">binsToBestSplit</span><span class="o">(</span><span class="n">aggStats</span><span class="o">,</span> <span class="n">splits</span><span class="o">,</span> <span class="n">featuresForNode</span><span class="o">,</span> <span class="n">nodes</span><span class="o">(</span><span class="n">nodeIndex</span><span class="o">))</span>
    <span class="o">(</span><span class="n">nodeIndex</span><span class="o">,</span> <span class="o">(</span><span class="n">split</span><span class="o">,</span> <span class="n">stats</span><span class="o">,</span> <span class="n">predict</span><span class="o">))</span>
<span class="o">}.</span><span class="n">collectAsMap</span><span class="o">()</span>
</code></pre>
</div>

<p>  该方法中的关键是对<code class="highlighter-rouge">binsToBestSplit</code>方法的调用，<code class="highlighter-rouge">binsToBestSplit</code>方法代码如下：</p>

<div class="language-scala highlighter-rouge"><pre class="highlight"><code><span class="k">private</span> <span class="k">def</span> <span class="n">binsToBestSplit</span><span class="o">(</span>
      <span class="n">binAggregates</span><span class="k">:</span> <span class="kt">DTStatsAggregator</span><span class="o">,</span>
      <span class="n">splits</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span class="kt">Array</span><span class="o">[</span><span class="kt">Split</span><span class="o">]],</span>
      <span class="n">featuresForNode</span><span class="k">:</span> <span class="kt">Option</span><span class="o">[</span><span class="kt">Array</span><span class="o">[</span><span class="kt">Int</span><span class="o">]],</span>
      <span class="n">node</span><span class="k">:</span> <span class="kt">Node</span><span class="o">)</span><span class="k">:</span> <span class="o">(</span><span class="kt">Split</span><span class="o">,</span> <span class="kt">InformationGainStats</span><span class="o">,</span> <span class="nc">Predict</span><span class="o">)</span> <span class="k">=</span> <span class="o">{</span>
    <span class="c1">// 如果当前节点是根节点，计算预测和不纯度
</span>    <span class="k">val</span> <span class="n">level</span> <span class="k">=</span> <span class="nc">Node</span><span class="o">.</span><span class="n">indexToLevel</span><span class="o">(</span><span class="n">node</span><span class="o">.</span><span class="n">id</span><span class="o">)</span>
    <span class="k">var</span> <span class="n">predictWithImpurity</span><span class="k">:</span> <span class="kt">Option</span><span class="o">[(</span><span class="kt">Predict</span>, <span class="kt">Double</span><span class="o">)]</span> <span class="k">=</span> <span class="k">if</span> <span class="o">(</span><span class="n">level</span> <span class="o">==</span> <span class="mi">0</span><span class="o">)</span> <span class="o">{</span>
      <span class="nc">None</span>
    <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
      <span class="nc">Some</span><span class="o">((</span><span class="n">node</span><span class="o">.</span><span class="n">predict</span><span class="o">,</span> <span class="n">node</span><span class="o">.</span><span class="n">impurity</span><span class="o">))</span>
    <span class="o">}</span>
    <span class="c1">// 对各特征及切分点，计算其信息增益并从中选择最优 (feature, split)
</span>    <span class="k">val</span> <span class="o">(</span><span class="n">bestSplit</span><span class="o">,</span> <span class="n">bestSplitStats</span><span class="o">)</span> <span class="k">=</span>
      <span class="nc">Range</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="n">binAggregates</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">numFeaturesPerNode</span><span class="o">).</span><span class="n">map</span> <span class="o">{</span> <span class="n">featureIndexIdx</span> <span class="k">=&gt;</span>
      <span class="k">val</span> <span class="n">featureIndex</span> <span class="k">=</span> <span class="k">if</span> <span class="o">(</span><span class="n">featuresForNode</span><span class="o">.</span><span class="n">nonEmpty</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">featuresForNode</span><span class="o">.</span><span class="n">get</span><span class="o">.</span><span class="n">apply</span><span class="o">(</span><span class="n">featureIndexIdx</span><span class="o">)</span>
      <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
        <span class="n">featureIndexIdx</span>
      <span class="o">}</span>
      <span class="k">val</span> <span class="n">numSplits</span> <span class="k">=</span> <span class="n">binAggregates</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">numSplits</span><span class="o">(</span><span class="n">featureIndex</span><span class="o">)</span>
       <span class="c1">//特征为连续值的情况
</span>      <span class="k">if</span> <span class="o">(</span><span class="n">binAggregates</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">isContinuous</span><span class="o">(</span><span class="n">featureIndex</span><span class="o">))</span> <span class="o">{</span>
        <span class="c1">// Cumulative sum (scanLeft) of bin statistics.
</span>        <span class="c1">// Afterwards, binAggregates for a bin is the sum of aggregates for
</span>        <span class="c1">// that bin + all preceding bins.
</span>        <span class="k">val</span> <span class="n">nodeFeatureOffset</span> <span class="k">=</span> <span class="n">binAggregates</span><span class="o">.</span><span class="n">getFeatureOffset</span><span class="o">(</span><span class="n">featureIndexIdx</span><span class="o">)</span>
        <span class="k">var</span> <span class="n">splitIndex</span> <span class="k">=</span> <span class="mi">0</span>
        <span class="k">while</span> <span class="o">(</span><span class="n">splitIndex</span> <span class="o">&lt;</span> <span class="n">numSplits</span><span class="o">)</span> <span class="o">{</span>
          <span class="n">binAggregates</span><span class="o">.</span><span class="n">mergeForFeature</span><span class="o">(</span><span class="n">nodeFeatureOffset</span><span class="o">,</span> <span class="n">splitIndex</span> <span class="o">+</span> <span class="mi">1</span><span class="o">,</span> <span class="n">splitIndex</span><span class="o">)</span>
          <span class="n">splitIndex</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="o">}</span>
        <span class="c1">// Find best split.
</span>        <span class="k">val</span> <span class="o">(</span><span class="n">bestFeatureSplitIndex</span><span class="o">,</span> <span class="n">bestFeatureGainStats</span><span class="o">)</span> <span class="k">=</span>
          <span class="nc">Range</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="n">numSplits</span><span class="o">).</span><span class="n">map</span> <span class="o">{</span> <span class="k">case</span> <span class="n">splitIdx</span> <span class="k">=&gt;</span>
            <span class="c1">//计算 leftChild 及 rightChild 子节点的 impurity
</span>            <span class="k">val</span> <span class="n">leftChildStats</span> <span class="k">=</span> <span class="n">binAggregates</span><span class="o">.</span><span class="n">getImpurityCalculator</span><span class="o">(</span><span class="n">nodeFeatureOffset</span><span class="o">,</span> <span class="n">splitIdx</span><span class="o">)</span>
            <span class="k">val</span> <span class="n">rightChildStats</span> <span class="k">=</span> <span class="n">binAggregates</span><span class="o">.</span><span class="n">getImpurityCalculator</span><span class="o">(</span><span class="n">nodeFeatureOffset</span><span class="o">,</span> <span class="n">numSplits</span><span class="o">)</span>
            <span class="n">rightChildStats</span><span class="o">.</span><span class="n">subtract</span><span class="o">(</span><span class="n">leftChildStats</span><span class="o">)</span>
            <span class="c1">//求 impurity 的预测值，采用的是平均值计算
</span>            <span class="n">predictWithImpurity</span> <span class="k">=</span> <span class="nc">Some</span><span class="o">(</span><span class="n">predictWithImpurity</span><span class="o">.</span><span class="n">getOrElse</span><span class="o">(</span>
              <span class="n">calculatePredictImpurity</span><span class="o">(</span><span class="n">leftChildStats</span><span class="o">,</span> <span class="n">rightChildStats</span><span class="o">)))</span>
            <span class="c1">//求信息增益 information gain 值，用于评估切分点是否最优,请参考决策树中1.4.4章节的介绍
</span>            <span class="k">val</span> <span class="n">gainStats</span> <span class="k">=</span> <span class="n">calculateGainForSplit</span><span class="o">(</span><span class="n">leftChildStats</span><span class="o">,</span>
              <span class="n">rightChildStats</span><span class="o">,</span> <span class="n">binAggregates</span><span class="o">.</span><span class="n">metadata</span><span class="o">,</span> <span class="n">predictWithImpurity</span><span class="o">.</span><span class="n">get</span><span class="o">.</span><span class="n">_2</span><span class="o">)</span>
            <span class="o">(</span><span class="n">splitIdx</span><span class="o">,</span> <span class="n">gainStats</span><span class="o">)</span>
          <span class="o">}.</span><span class="n">maxBy</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">_2</span><span class="o">.</span><span class="n">gain</span><span class="o">)</span>
        <span class="o">(</span><span class="n">splits</span><span class="o">(</span><span class="n">featureIndex</span><span class="o">)(</span><span class="n">bestFeatureSplitIndex</span><span class="o">),</span> <span class="n">bestFeatureGainStats</span><span class="o">)</span>
      <span class="o">}</span>
      <span class="c1">//无序离散特征时的情况
</span>      <span class="k">else</span> <span class="k">if</span> <span class="o">(</span><span class="n">binAggregates</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">isUnordered</span><span class="o">(</span><span class="n">featureIndex</span><span class="o">))</span> <span class="o">{</span>
        <span class="c1">// Unordered categorical feature
</span>        <span class="k">val</span> <span class="o">(</span><span class="n">leftChildOffset</span><span class="o">,</span> <span class="n">rightChildOffset</span><span class="o">)</span> <span class="k">=</span>
          <span class="n">binAggregates</span><span class="o">.</span><span class="n">getLeftRightFeatureOffsets</span><span class="o">(</span><span class="n">featureIndexIdx</span><span class="o">)</span>
        <span class="k">val</span> <span class="o">(</span><span class="n">bestFeatureSplitIndex</span><span class="o">,</span> <span class="n">bestFeatureGainStats</span><span class="o">)</span> <span class="k">=</span>
          <span class="nc">Range</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="n">numSplits</span><span class="o">).</span><span class="n">map</span> <span class="o">{</span> <span class="n">splitIndex</span> <span class="k">=&gt;</span>
            <span class="k">val</span> <span class="n">leftChildStats</span> <span class="k">=</span> <span class="n">binAggregates</span><span class="o">.</span><span class="n">getImpurityCalculator</span><span class="o">(</span><span class="n">leftChildOffset</span><span class="o">,</span> <span class="n">splitIndex</span><span class="o">)</span>
            <span class="k">val</span> <span class="n">rightChildStats</span> <span class="k">=</span> <span class="n">binAggregates</span><span class="o">.</span><span class="n">getImpurityCalculator</span><span class="o">(</span><span class="n">rightChildOffset</span><span class="o">,</span> <span class="n">splitIndex</span><span class="o">)</span>
            <span class="n">predictWithImpurity</span> <span class="k">=</span> <span class="nc">Some</span><span class="o">(</span><span class="n">predictWithImpurity</span><span class="o">.</span><span class="n">getOrElse</span><span class="o">(</span>
              <span class="n">calculatePredictImpurity</span><span class="o">(</span><span class="n">leftChildStats</span><span class="o">,</span> <span class="n">rightChildStats</span><span class="o">)))</span>
            <span class="k">val</span> <span class="n">gainStats</span> <span class="k">=</span> <span class="n">calculateGainForSplit</span><span class="o">(</span><span class="n">leftChildStats</span><span class="o">,</span>
              <span class="n">rightChildStats</span><span class="o">,</span> <span class="n">binAggregates</span><span class="o">.</span><span class="n">metadata</span><span class="o">,</span> <span class="n">predictWithImpurity</span><span class="o">.</span><span class="n">get</span><span class="o">.</span><span class="n">_2</span><span class="o">)</span>
            <span class="o">(</span><span class="n">splitIndex</span><span class="o">,</span> <span class="n">gainStats</span><span class="o">)</span>
          <span class="o">}.</span><span class="n">maxBy</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">_2</span><span class="o">.</span><span class="n">gain</span><span class="o">)</span>
        <span class="o">(</span><span class="n">splits</span><span class="o">(</span><span class="n">featureIndex</span><span class="o">)(</span><span class="n">bestFeatureSplitIndex</span><span class="o">),</span> <span class="n">bestFeatureGainStats</span><span class="o">)</span>
      <span class="o">}</span> <span class="k">else</span> <span class="o">{</span><span class="c1">//有序离散特征时的情况
</span>        <span class="c1">// Ordered categorical feature
</span>        <span class="k">val</span> <span class="n">nodeFeatureOffset</span> <span class="k">=</span> <span class="n">binAggregates</span><span class="o">.</span><span class="n">getFeatureOffset</span><span class="o">(</span><span class="n">featureIndexIdx</span><span class="o">)</span>
        <span class="k">val</span> <span class="n">numBins</span> <span class="k">=</span> <span class="n">binAggregates</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">numBins</span><span class="o">(</span><span class="n">featureIndex</span><span class="o">)</span>
        <span class="cm">/* Each bin is one category (feature value).
         * The bins are ordered based on centroidForCategories, and this ordering determines which
         * splits are considered.  (With K categories, we consider K - 1 possible splits.)
         *
         * centroidForCategories is a list: (category, centroid)
         */</span>
        <span class="c1">//多元分类时的情况
</span>        <span class="k">val</span> <span class="n">centroidForCategories</span> <span class="k">=</span> <span class="k">if</span> <span class="o">(</span><span class="n">binAggregates</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">isMulticlass</span><span class="o">)</span> <span class="o">{</span>
          <span class="c1">// For categorical variables in multiclass classification,
</span>          <span class="c1">// the bins are ordered by the impurity of their corresponding labels.
</span>          <span class="nc">Range</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="n">numBins</span><span class="o">).</span><span class="n">map</span> <span class="o">{</span> <span class="k">case</span> <span class="n">featureValue</span> <span class="k">=&gt;</span>
            <span class="k">val</span> <span class="n">categoryStats</span> <span class="k">=</span> <span class="n">binAggregates</span><span class="o">.</span><span class="n">getImpurityCalculator</span><span class="o">(</span><span class="n">nodeFeatureOffset</span><span class="o">,</span> <span class="n">featureValue</span><span class="o">)</span>
            <span class="k">val</span> <span class="n">centroid</span> <span class="k">=</span> <span class="k">if</span> <span class="o">(</span><span class="n">categoryStats</span><span class="o">.</span><span class="n">count</span> <span class="o">!=</span> <span class="mi">0</span><span class="o">)</span> <span class="o">{</span>
              <span class="c1">// impurity 求的就是均方差
</span>              <span class="n">categoryStats</span><span class="o">.</span><span class="n">calculate</span><span class="o">()</span>
            <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
              <span class="nc">Double</span><span class="o">.</span><span class="nc">MaxValue</span>
            <span class="o">}</span>
            <span class="o">(</span><span class="n">featureValue</span><span class="o">,</span> <span class="n">centroid</span><span class="o">)</span>
          <span class="o">}</span>
        <span class="o">}</span> <span class="k">else</span> <span class="o">{</span> <span class="c1">// 回归或二元分类时的情况
</span>          <span class="c1">// For categorical variables in regression and binary classification,
</span>          <span class="c1">// the bins are ordered by the centroid of their corresponding labels.
</span>          <span class="nc">Range</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="n">numBins</span><span class="o">).</span><span class="n">map</span> <span class="o">{</span> <span class="k">case</span> <span class="n">featureValue</span> <span class="k">=&gt;</span>
            <span class="k">val</span> <span class="n">categoryStats</span> <span class="k">=</span> <span class="n">binAggregates</span><span class="o">.</span><span class="n">getImpurityCalculator</span><span class="o">(</span><span class="n">nodeFeatureOffset</span><span class="o">,</span> <span class="n">featureValue</span><span class="o">)</span>
            <span class="k">val</span> <span class="n">centroid</span> <span class="k">=</span> <span class="k">if</span> <span class="o">(</span><span class="n">categoryStats</span><span class="o">.</span><span class="n">count</span> <span class="o">!=</span> <span class="mi">0</span><span class="o">)</span> <span class="o">{</span>
              <span class="c1">//求的就是平均值作为 impurity
</span>              <span class="n">categoryStats</span><span class="o">.</span><span class="n">predict</span>
            <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
              <span class="nc">Double</span><span class="o">.</span><span class="nc">MaxValue</span>
            <span class="o">}</span>
            <span class="o">(</span><span class="n">featureValue</span><span class="o">,</span> <span class="n">centroid</span><span class="o">)</span>
          <span class="o">}</span>
        <span class="o">}</span>
        <span class="c1">// bins sorted by centroids
</span>        <span class="k">val</span> <span class="n">categoriesSortedByCentroid</span> <span class="k">=</span> <span class="n">centroidForCategories</span><span class="o">.</span><span class="n">toList</span><span class="o">.</span><span class="n">sortBy</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">_2</span><span class="o">)</span>
        <span class="c1">// Cumulative sum (scanLeft) of bin statistics.
</span>        <span class="c1">// Afterwards, binAggregates for a bin is the sum of aggregates for
</span>        <span class="c1">// that bin + all preceding bins.
</span>        <span class="k">var</span> <span class="n">splitIndex</span> <span class="k">=</span> <span class="mi">0</span>
        <span class="k">while</span> <span class="o">(</span><span class="n">splitIndex</span> <span class="o">&lt;</span> <span class="n">numSplits</span><span class="o">)</span> <span class="o">{</span>
          <span class="k">val</span> <span class="n">currentCategory</span> <span class="k">=</span> <span class="n">categoriesSortedByCentroid</span><span class="o">(</span><span class="n">splitIndex</span><span class="o">).</span><span class="n">_1</span>
          <span class="k">val</span> <span class="n">nextCategory</span> <span class="k">=</span> <span class="n">categoriesSortedByCentroid</span><span class="o">(</span><span class="n">splitIndex</span> <span class="o">+</span> <span class="mi">1</span><span class="o">).</span><span class="n">_1</span>
          <span class="c1">//将两个箱子的状态信息进行合并
</span>          <span class="n">binAggregates</span><span class="o">.</span><span class="n">mergeForFeature</span><span class="o">(</span><span class="n">nodeFeatureOffset</span><span class="o">,</span> <span class="n">nextCategory</span><span class="o">,</span> <span class="n">currentCategory</span><span class="o">)</span>
          <span class="n">splitIndex</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="o">}</span>
        <span class="c1">// lastCategory = index of bin with total aggregates for this (node, feature)
</span>        <span class="k">val</span> <span class="n">lastCategory</span> <span class="k">=</span> <span class="n">categoriesSortedByCentroid</span><span class="o">.</span><span class="n">last</span><span class="o">.</span><span class="n">_1</span>
        <span class="c1">// Find best split.
</span>        <span class="c1">//通过信息增益值选择最优切分点
</span>        <span class="k">val</span> <span class="o">(</span><span class="n">bestFeatureSplitIndex</span><span class="o">,</span> <span class="n">bestFeatureGainStats</span><span class="o">)</span> <span class="k">=</span>
          <span class="nc">Range</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="n">numSplits</span><span class="o">).</span><span class="n">map</span> <span class="o">{</span> <span class="n">splitIndex</span> <span class="k">=&gt;</span>
            <span class="k">val</span> <span class="n">featureValue</span> <span class="k">=</span> <span class="n">categoriesSortedByCentroid</span><span class="o">(</span><span class="n">splitIndex</span><span class="o">).</span><span class="n">_1</span>
            <span class="k">val</span> <span class="n">leftChildStats</span> <span class="k">=</span>
              <span class="n">binAggregates</span><span class="o">.</span><span class="n">getImpurityCalculator</span><span class="o">(</span><span class="n">nodeFeatureOffset</span><span class="o">,</span> <span class="n">featureValue</span><span class="o">)</span>
            <span class="k">val</span> <span class="n">rightChildStats</span> <span class="k">=</span>
              <span class="n">binAggregates</span><span class="o">.</span><span class="n">getImpurityCalculator</span><span class="o">(</span><span class="n">nodeFeatureOffset</span><span class="o">,</span> <span class="n">lastCategory</span><span class="o">)</span>
            <span class="n">rightChildStats</span><span class="o">.</span><span class="n">subtract</span><span class="o">(</span><span class="n">leftChildStats</span><span class="o">)</span>
            <span class="n">predictWithImpurity</span> <span class="k">=</span> <span class="nc">Some</span><span class="o">(</span><span class="n">predictWithImpurity</span><span class="o">.</span><span class="n">getOrElse</span><span class="o">(</span>
              <span class="n">calculatePredictImpurity</span><span class="o">(</span><span class="n">leftChildStats</span><span class="o">,</span> <span class="n">rightChildStats</span><span class="o">)))</span>
            <span class="k">val</span> <span class="n">gainStats</span> <span class="k">=</span> <span class="n">calculateGainForSplit</span><span class="o">(</span><span class="n">leftChildStats</span><span class="o">,</span>
              <span class="n">rightChildStats</span><span class="o">,</span> <span class="n">binAggregates</span><span class="o">.</span><span class="n">metadata</span><span class="o">,</span> <span class="n">predictWithImpurity</span><span class="o">.</span><span class="n">get</span><span class="o">.</span><span class="n">_2</span><span class="o">)</span>
            <span class="o">(</span><span class="n">splitIndex</span><span class="o">,</span> <span class="n">gainStats</span><span class="o">)</span>
          <span class="o">}.</span><span class="n">maxBy</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">_2</span><span class="o">.</span><span class="n">gain</span><span class="o">)</span>
        <span class="k">val</span> <span class="n">categoriesForSplit</span> <span class="k">=</span>
          <span class="n">categoriesSortedByCentroid</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">_1</span><span class="o">.</span><span class="n">toDouble</span><span class="o">).</span><span class="n">slice</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="n">bestFeatureSplitIndex</span> <span class="o">+</span> <span class="mi">1</span><span class="o">)</span>
        <span class="k">val</span> <span class="n">bestFeatureSplit</span> <span class="k">=</span>
          <span class="k">new</span> <span class="nc">Split</span><span class="o">(</span><span class="n">featureIndex</span><span class="o">,</span> <span class="nc">Double</span><span class="o">.</span><span class="nc">MinValue</span><span class="o">,</span> <span class="nc">Categorical</span><span class="o">,</span> <span class="n">categoriesForSplit</span><span class="o">)</span>
        <span class="o">(</span><span class="n">bestFeatureSplit</span><span class="o">,</span> <span class="n">bestFeatureGainStats</span><span class="o">)</span>
      <span class="o">}</span>
    <span class="o">}.</span><span class="n">maxBy</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">_2</span><span class="o">.</span><span class="n">gain</span><span class="o">)</span>
    <span class="o">(</span><span class="n">bestSplit</span><span class="o">,</span> <span class="n">bestSplitStats</span><span class="o">,</span> <span class="n">predictWithImpurity</span><span class="o">.</span><span class="n">get</span><span class="o">.</span><span class="n">_1</span><span class="o">)</span>
  <span class="o">}</span>
</code></pre>
</div>

<h3 id="section-8">5.2 预测分析</h3>

<p>  在利用随机森林进行预测时，调用的<code class="highlighter-rouge">predict</code>方法扩展自<code class="highlighter-rouge">TreeEnsembleModel</code>，它是树结构组合模型的表示，其核心代码如下所示：</p>

<div class="language-scala highlighter-rouge"><pre class="highlight"><code><span class="c1">//不同的策略采用不同的预测方法
</span><span class="k">def</span> <span class="n">predict</span><span class="o">(</span><span class="n">features</span><span class="k">:</span> <span class="kt">Vector</span><span class="o">)</span><span class="k">:</span> <span class="kt">Double</span> <span class="o">=</span> <span class="o">{</span>
    <span class="o">(</span><span class="n">algo</span><span class="o">,</span> <span class="n">combiningStrategy</span><span class="o">)</span> <span class="k">match</span> <span class="o">{</span>
      <span class="k">case</span> <span class="o">(</span><span class="nc">Regression</span><span class="o">,</span> <span class="nc">Sum</span><span class="o">)</span> <span class="k">=&gt;</span>
        <span class="n">predictBySumming</span><span class="o">(</span><span class="n">features</span><span class="o">)</span>
      <span class="k">case</span> <span class="o">(</span><span class="nc">Regression</span><span class="o">,</span> <span class="nc">Average</span><span class="o">)</span> <span class="k">=&gt;</span>
        <span class="n">predictBySumming</span><span class="o">(</span><span class="n">features</span><span class="o">)</span> <span class="o">/</span> <span class="n">sumWeights</span>
      <span class="k">case</span> <span class="o">(</span><span class="nc">Classification</span><span class="o">,</span> <span class="nc">Sum</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="c1">// binary classification
</span>        <span class="k">val</span> <span class="n">prediction</span> <span class="k">=</span> <span class="n">predictBySumming</span><span class="o">(</span><span class="n">features</span><span class="o">)</span>
        <span class="c1">// TODO: predicted labels are +1 or -1 for GBT. Need a better way to store this info.
</span>        <span class="k">if</span> <span class="o">(</span><span class="n">prediction</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="o">)</span> <span class="mf">1.0</span> <span class="k">else</span> <span class="mf">0.0</span>
      <span class="k">case</span> <span class="o">(</span><span class="nc">Classification</span><span class="o">,</span> <span class="nc">Vote</span><span class="o">)</span> <span class="k">=&gt;</span>
        <span class="n">predictByVoting</span><span class="o">(</span><span class="n">features</span><span class="o">)</span>
      <span class="k">case</span> <span class="k">_</span> <span class="k">=&gt;</span>
        <span class="k">throw</span> <span class="k">new</span> <span class="nc">IllegalArgumentException</span><span class="o">()</span>
    <span class="o">}</span>
  <span class="o">}</span>
<span class="k">private</span> <span class="k">def</span> <span class="n">predictBySumming</span><span class="o">(</span><span class="n">features</span><span class="k">:</span> <span class="kt">Vector</span><span class="o">)</span><span class="k">:</span> <span class="kt">Double</span> <span class="o">=</span> <span class="o">{</span>
    <span class="k">val</span> <span class="n">treePredictions</span> <span class="k">=</span> <span class="n">trees</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">predict</span><span class="o">(</span><span class="n">features</span><span class="o">))</span>
    <span class="c1">//两个向量的内集
</span>    <span class="n">blas</span><span class="o">.</span><span class="n">ddot</span><span class="o">(</span><span class="n">numTrees</span><span class="o">,</span> <span class="n">treePredictions</span><span class="o">,</span> <span class="mi">1</span><span class="o">,</span> <span class="n">treeWeights</span><span class="o">,</span> <span class="mi">1</span><span class="o">)</span>
<span class="o">}</span>
<span class="c1">//通过投票选举
</span><span class="k">private</span> <span class="k">def</span> <span class="n">predictByVoting</span><span class="o">(</span><span class="n">features</span><span class="k">:</span> <span class="kt">Vector</span><span class="o">)</span><span class="k">:</span> <span class="kt">Double</span> <span class="o">=</span> <span class="o">{</span>
    <span class="k">val</span> <span class="n">votes</span> <span class="k">=</span> <span class="n">mutable</span><span class="o">.</span><span class="nc">Map</span><span class="o">.</span><span class="n">empty</span><span class="o">[</span><span class="kt">Int</span>, <span class="kt">Double</span><span class="o">]</span>
    <span class="n">trees</span><span class="o">.</span><span class="n">view</span><span class="o">.</span><span class="n">zip</span><span class="o">(</span><span class="n">treeWeights</span><span class="o">).</span><span class="n">foreach</span> <span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="n">tree</span><span class="o">,</span> <span class="n">weight</span><span class="o">)</span> <span class="k">=&gt;</span>
      <span class="k">val</span> <span class="n">prediction</span> <span class="k">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">predict</span><span class="o">(</span><span class="n">features</span><span class="o">).</span><span class="n">toInt</span>
      <span class="n">votes</span><span class="o">(</span><span class="n">prediction</span><span class="o">)</span> <span class="k">=</span> <span class="n">votes</span><span class="o">.</span><span class="n">getOrElse</span><span class="o">(</span><span class="n">prediction</span><span class="o">,</span> <span class="mf">0.0</span><span class="o">)</span> <span class="o">+</span> <span class="n">weight</span>
    <span class="o">}</span>
    <span class="n">votes</span><span class="o">.</span><span class="n">maxBy</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">_2</span><span class="o">).</span><span class="n">_1</span>
<span class="o">}</span>
</code></pre>
</div>

<h1 id="section-9">参考文献</h1>

<p>【1】机器学习.周志华</p>

<p>【2】<a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-spark-random-forest/">Spark 随机森林算法原理、源码分析及案例实战</a></p>

<p>【3】<a href="https://spark-summit.org/wp-content/uploads/2014/07/Scalable-Distributed-Decision-Trees-in-Spark-Made-Das-Sparks-Talwalkar.pdf">Scalable Distributed Decision Trees in Spark MLlib</a></p>


                <hr>


                <ul class="pager">
                    
                    <li class="previous">
                        <a href="/mladvance/2017/04/06/gbts/" data-toggle="tooltip" data-placement="top" title="Spark-ML-0304-Ensembles-Gradient-Boosted Trees (GBTs)">
                        Previous<br>
                        <span>Spark-ML-0304-Ensembles-Gradient-Boosted Trees (GBTs)</span>
                        </a>
                    </li>
                    
                    
                    <li class="next">
                        <a href="/mladvance/2017/04/06/isotonic-regression/" data-toggle="tooltip" data-placement="top" title="Spark-ML-0305-Isotonic regression">
                        Next<br>
                        <span>Spark-ML-0305-Isotonic regression</span>
                        </a>
                    </li>
                    
                </ul>
                    <div class="ds-share flat"
                    <div class="ds-thread"
                        data-thread-key="/mladvance/2017/04/06/random-forests"
                        data-title="Spark-ML-0304-Ensembles-RF"
                        data-url="http://machinelearningadvance.com/mladvance/2017/04/06/random-forests/" >
                    <div class="ds-share-inline">
                      <ul  class="ds-share-icons-16">

                        <li data-toggle="ds-share-icons-more"><a class="ds-more" href="javascript:void(0);">分享到：</a></li>
                        <li><a class="ds-weibo" href="javascript:void(0);" data-service="weibo">微博</a></li>
                        <li><a class="ds-qzone" href="javascript:void(0);" data-service="qzone">QQ空间</a></li>
                        <li><a class="ds-qqt" href="javascript:void(0);" data-service="qqt">腾讯微博</a></li>
                        <li><a class="ds-wechat" href="javascript:void(0);" data-service="wechat">微信</a></li>

                      </ul>
                      <div class="ds-share-icons-more">
                      </div>
                    </div>
                 </div>
                
                <!-- 多说评论框 start -->
                <div class="comment">
                    <div class="ds-thread"
                        data-thread-key="/mladvance/2017/04/06/random-forests"
                        data-title="Spark-ML-0304-Ensembles-RF"
                        data-url="http://machinelearningadvance.com/mladvance/2017/04/06/random-forests/" >
                    </div>

                </div>
                <!-- 多说评论框 end -->
                

                
                <!-- disqus 评论框 start -->
                <div class="comment">
                    <div id="disqus_thread" class="disqus-thread"></div>
                </div>
                <!-- disqus 评论框 end -->
                

            </div>

    <!-- Side Catalog Container -->
        
            <div class="
                col-lg-2 col-lg-offset-0
                visible-lg-block
                sidebar-container
                catalog-container">
                <div class="side-catalog">
                    <hr class="hidden-sm hidden-xs">
                    <h5>
                        <a class="catalog-toggle" href="#">CATALOG</a>
                    </h5>
                    <ul class="catalog-body"></ul>
                </div>
            </div>
        

    <!-- Sidebar Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                
                <section>
                    <hr class="hidden-sm hidden-xs">
                    <h5><a href="/tags/">FEATURED TAGS</a></h5>
                    <div class="tags">
        				
                            
                				<a href="/tags/#Machine Learning" title="Machine Learning" rel="10">
                                Machine Learning
                                </a>
                            
        				
                            
        				
                            
                				<a href="/tags/#Python" title="Python" rel="9">
                                Python
                                </a>
                            
        				
        			</div>
                </section>
                

                <!-- Friends Blog -->
                
                <hr>
                <h5>FRIENDS</h5>
                <ul class="list-inline">
                    
                        <li><a href="https://www.kaggle.com/">Your Home for Data Science</a></li>
                    
                        <li><a href="https://www.analyticsvidhya.com/">Analytics Vidhya</a></li>
                    
                        <li><a href="https://www.cfainstitute.org/pages/index.aspx">CFA</a></li>
                    
                        <li><a href="http://www.garp.org/#!/home">GARP</a></li>
                    
                        <li><a href="http://www.investopedia.com/">Investopedia</a></li>
                    
                        <li><a href="https://www.quantstart.com/">Quant Start</a></li>
                    
                        <li><a href="http://muchong.com/bbs/">muchong</a></li>
                    
                        <li><a href="https://www.coursera.org/learn/machine-learning">Coursera</a></li>
                    
                </ul>
                
            </div>
        </div>
    </div>
</article>
<script type="text/javascript"  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


<!-- 多说公共JS代码 start (一个网页只需插入一次) -->
<!-- 多说评论框 end -->
<!-- 多说公共JS代码 start (一个网页只需插入一次) -->
<script type="text/javascript">
var duoshuoQuery = {short_name:"machinelearningadvance"};
    (function() {
        var ds = document.createElement('script');
        ds.type = 'text/javascript';ds.async = true;
        ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
        ds.charset = 'UTF-8';
        (document.getElementsByTagName('head')[0]
         || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
    </script>
<!-- 多说公共JS代码 end -->
<!-- 多说公共JS代码 end -->




<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("http://cdn.bootcss.com/anchor-js/1.1.1/anchor.min.js",function(){
        anchors.options = {
          visible: 'always',
          placement: 'right',
          icon: '#'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>
<style>
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>



    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                    
                    <li>
                        <a href="/feed.xml">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-rss fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    
                    

                    <!-- add Weibo, Zhihu by Hux, add target = "_blank" to <a> by Hux -->
                    
                    
                    <li>
                        <a target="_blank" href="http://weibo.com/u/2672280861">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-weibo fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    


                    
                    <li>
                        <a target="_blank" href="https://www.facebook.com/davidyjun">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-facebook fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li>
                        <a target="_blank" href="https://github.com/helloourworld">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li>
                        <a target="_blank" href="https://www.linkedin.com/in/lijun-yu-13b9b475">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    
                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; Big Data Memo 2017
                </p>
            </div>
        </div>
    </div>
</footer>

<!-- jQuery -->
<script src="/js/jquery.min.js "></script>

<!-- Bootstrap Core JavaScript -->
<script src="/js/bootstrap.min.js "></script>

<!-- Custom Theme JavaScript -->
<script src="/js/hux-blog.min.js "></script>


<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>

<!--
     Because of the native support for backtick-style fenced code blocks
     right within the Markdown is landed in Github Pages,
     From V1.6, There is no need for Highlight.js,
     so Huxblog drops it officially.

     - https://github.com/blog/2100-github-pages-now-faster-and-simpler-with-jekyll-3-0
     - https://help.github.com/articles/creating-and-highlighting-code-blocks/
     - https://github.com/jneen/rouge/wiki/list-of-supported-languages-and-lexers
-->
<!--
    <script>
        async("http://cdn.bootcss.com/highlight.js/8.6/highlight.min.js", function(){
            hljs.initHighlightingOnLoad();
        })
    </script>
    <link href="http://cdn.bootcss.com/highlight.js/8.6/styles/github.min.css" rel="stylesheet">
-->


<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async('/js/jquery.tagcloud.js',function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("http://cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->

<script>
    // dynamic User by Hux
    var _gaId = 'UA-82819752-1';
    var _gaDomain = 'machinelearningadvance.com';

    // Originial
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', _gaId, _gaDomain);
    ga('send', 'pageview');
</script>



<!-- Baidu Tongji -->

<script>
    // dynamic User by Hux
    var _baId = '863b1f46c83a8e14e47782106aee7e7f';

    // Originial
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?" + _baId;
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
</script>




<!-- Side Catalog -->

<script type="text/javascript">
    function generateCatalog (selector) {
        var P = $('div.post-container'),a,n,t,l,i,c;
        a = P.find('h1,h2,h3,h4,h5,h6');
        a.each(function () {
            n = $(this).prop('tagName').toLowerCase();
            i = "#"+$(this).prop('id');
            t = $(this).text();
            c = $('<a href="'+i+'" rel="nofollow">'+t+'</a>');
            l = $('<li class="'+n+'_nav"></li>').append(c);
            $(selector).append(l);
        });
        return true;
    }

    generateCatalog(".catalog-body");

    // toggle side catalog
    $(".catalog-toggle").click((function(e){
        e.preventDefault();
        $('.side-catalog').toggleClass("fold")
    }))

    /*
     * Doc: https://github.com/davist11/jQuery-One-Page-Nav
     * Fork by Hux to support padding
     */
    async("/js/jquery.nav.js", function () {
        $('.catalog-body').onePageNav({
            currentClass: "active",
            changeHash: !1,
            easing: "swing",
            filter: "",
            scrollSpeed: 700,
            scrollOffset: 0,
            scrollThreshold: .2,
            begin: null,
            end: null,
            scrollChange: null,
            padding: 80
        });
    });
</script>





<!-- Image to hack wechat -->
<img src="/images/background.jpg" width="0" height="0" />
<!-- Migrate from head to bottom, no longer block render and still work -->

</body>

</html>
